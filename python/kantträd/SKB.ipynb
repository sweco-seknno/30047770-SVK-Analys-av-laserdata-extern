{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Trees Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEKNNO.json\"\n",
    "#settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEVPLI.json\"\n",
    "#settings_file = r\"C:\\SVK_utveckling\\settings_SEVPLI.json\"\n",
    "settings_file = r\"C:\\Users\\SE1K4H\\Desktop\\SVK-Analys-Filer\\settings_SEKNNO.json\" # Working on Elsas's computer\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(settings_file):\n",
    "        raise FileNotFoundError(f\"Could not find {settings_file}\")\n",
    "\n",
    "    with open(settings_file, 'r', encoding='utf-8') as file:\n",
    "        settings = json.load(file)\n",
    "\n",
    "    # TODO: check which ones are needed and not\n",
    "    run_ID = settings[\"run_ID\"]\n",
    "    powerline_list = settings[\"powerline_list\"]\n",
    "    local_dir = settings[\"local_folder\"]\n",
    "    wires_gdb_template = settings[\"wires_gdb_template\"]\n",
    "    domains_folder = settings[\"domains_folder\"]\n",
    "    scandate_file = settings[\"scandate_file\"]\n",
    "    cvd_LEDNINGSGATA_path = os.path.join(domains_folder, \"cvd_LEDNINGSGATA.txt\")\n",
    "    powerlines_folder = settings[\"powerlines_folder\"]\n",
    "    module_path = settings[\"modules\"] \n",
    "    ogr2ogr_path = settings[\"ogr2ogr_path\"]   \n",
    "    proj_lib_path = settings[\"proj_lib_path\"]\n",
    "    gdal_data_path = settings[\"gdal_data_path\"]\n",
    "    DEFAULT_DB_NAME = settings[\"default_db_name\"]\n",
    "    DB_NAME = settings[\"db_name\"]\n",
    "    USER = settings[\"db_user\"]\n",
    "    PASSWORD = settings[\"db_password\"]\n",
    "    HOST = settings[\"host\"]\n",
    "    PORT = settings[\"port\"] \n",
    "\n",
    "    if None in (run_ID, powerline_list, local_dir, wires_gdb_template, domains_folder, scandate_file, powerlines_folder, module_path, ogr2ogr_path, proj_lib_path, gdal_data_path, DEFAULT_DB_NAME, DB_NAME, USER, PASSWORD, HOST, PORT):\n",
    "        raise KeyError(f\"One or more keys are missing in {settings_file}\")\n",
    "    \n",
    "    print(f\"Settings loaded successfully.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Invalid JSON format in {settings_file}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PostGIS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# TODO can move these kind of methods to utils or similiar?\n",
    "def connect_to_database(db_name):\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname=db_name, user=USER, password=PASSWORD, host=HOST, port=PORT)\n",
    "        conn.autocommit = True\n",
    "        return conn\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to {db_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def execute_query(conn, query, data=None):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query, data or ())\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge SKB text files for all blocks of a powerline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import fileinput\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# TODO add exception handling.\n",
    "\n",
    "def combine_blocks(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    \n",
    "    block_dir = os.path.join(line_dir, \"kantträd\", \"block\")\n",
    "    combined_blocks_path = os.path.join(line_dir, \"kantträd\", \"SKB_raw.txt\")\n",
    "    \n",
    "    # Merge files for all blocks into one\n",
    "    merge_blocks(block_dir, \"*.txt\", combined_blocks_path)\n",
    "    print(f\"Successfully merged all edge trees blocks for {LG}/line_{line} into text file {combined_blocks_path}.\")\n",
    "\n",
    "def merge_blocks(src_dir, search_pattern, dst_file):\n",
    "    blocks = glob.glob(os.path.join(src_dir, search_pattern))\n",
    "    with open(dst_file, \"w\") as fh:\n",
    "        input_lines = fileinput.input(blocks)\n",
    "        fh.writelines(input_lines)\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "powerlines_df.apply(combine_blocks, axis=1)\n",
    "print(f\"Done with all power lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Edge Trees to PostGIS Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from psycopg2 import sql #TODO update to and install psycopg, the latest version.\n",
    "\n",
    "cur = None\n",
    "conn = None\n",
    "\n",
    "def create_SKB_table(row, conn_db):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    SKB_dir = line_dir / \"kantträd\"\n",
    "    SKB_file_in = os.path.join(SKB_dir, f\"SKB_raw.txt\")\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name))\n",
    "    create_query = sql.SQL(\"CREATE TABLE {table_name}(objectid SERIAL PRIMARY KEY, x DOUBLE PRECISION, y DOUBLE PRECISION, z FLOAT, dz FLOAT, mz FLOAT, shape geometry(POINTZ, 3006))\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create table for powerline\n",
    "        execute_query(conn_db, drop_table_if_exists_query)\n",
    "        execute_query(conn_db, create_query)\n",
    "        \n",
    "        # Step 2: Insert edge trees into powerline table \n",
    "        with open(SKB_file_in, 'r') as src_file:\n",
    "            for file_line in src_file:\n",
    "                l_split = file_line.split(' ')\n",
    "                x = float(l_split[0])\n",
    "                y = float(l_split[1])\n",
    "                z = float(l_split[2])\n",
    "                dz = float(l_split[3])\n",
    "                mz = z - dz\n",
    "                \n",
    "                # TODO Börja här, kika på formatet för x, y, z osv\n",
    "                insert_query = sql.SQL(\"INSERT INTO {table_name} (x, y, z, dz, mz, shape) VALUES ({x}, {y}, {z}, {dz}, {mz}, ST_GeomFromText('POINTZ({x} {y} {mz})', 3006));\").format(\n",
    "                    table_name=sql.Identifier(table_name),\n",
    "                    x=sql.Placeholder(\"x\"),\n",
    "                    y=sql.Placeholder(\"y\"),\n",
    "                    z=sql.Placeholder(\"z\"),\n",
    "                    dz=sql.Placeholder(\"dz\"),\n",
    "                    mz=sql.Placeholder(\"mz\"))\n",
    "                tree_data = {\"x\": x, \"y\": y, \"z\": z, \"dz\": dz, \"mz\": mz}\n",
    "                execute_query(conn_db, insert_query, tree_data)\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(lambda row: create_SKB_table(row, conn_db), axis=1)\n",
    "    \n",
    "    print(\"Successfully inserted edge trees to PostGIS database.\") \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_distances(row, conn_db):\n",
    "    #TODO se över SQL query om de kan förbättras, optimeras\n",
    "    #TODO Borde ta bort columner först om de existerar \n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    wire_table_name = f\"{LG.lower()}_{line}_fas\"\n",
    "    \n",
    "    add_columns_query = sql.SQL(\"\"\"ALTER TABLE {table_name}\n",
    "        ADD COLUMN avst_mz_fas DOUBLE PRECISION,\n",
    "        ADD COLUMN avst_hori DOUBLE PRECISION, \n",
    "        ADD COLUMN avst_fas DOUBLE PRECISION;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "    \n",
    "    avst_mz_fas_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_mz_fas = subquery.distance\n",
    "        FROM (\n",
    "            SELECT DISTINCT ON (p.objectid)  \n",
    "                p.objectid,\n",
    "                ST_3DDistance(p.shape, l.shape) AS distance\n",
    "            FROM \n",
    "                {table_name} p, \n",
    "                {wire_table_name} l\n",
    "            WHERE \n",
    "                ST_3DDWithin(p.shape, l.shape, 100)\n",
    "            ORDER BY \n",
    "                p.objectid, distance ASC\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name), wire_table_name=sql.Identifier(wire_table_name))\n",
    "\n",
    "    avst_fas_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_fas = subquery.distance\n",
    "        FROM (\n",
    "            SELECT objectid, avst_mz_fas - dz AS distance\n",
    "            FROM {table_name}\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    avst_hori_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_hori = subquery.distance\n",
    "        FROM (\n",
    "            SELECT DISTINCT ON (p.objectid)  \n",
    "                p.objectid,\n",
    "                ST_Distance(p.shape, l.shape) AS distance\n",
    "            FROM \n",
    "                {table_name} p, \n",
    "                {wire_table_name} l\n",
    "            WHERE \n",
    "                ST_DWithin(p.shape, l.shape, 100)\n",
    "            ORDER BY \n",
    "                p.objectid, distance ASC\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name), wire_table_name=sql.Identifier(wire_table_name))\n",
    "\n",
    "    # Step 1: Add attributes\n",
    "    execute_query(conn_db, add_columns_query)\n",
    "    print(f\"Columns where added successfully to table {table_name}\")\n",
    "\n",
    "    # Step 2: Calculate shortest distance between mz and phase.\n",
    "    execute_query(conn_db, avst_mz_fas_query)\n",
    "    print(f\"avst_mz_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 3: Calculate the shortest distance between tree top (z) and phase with event of potential fall.\n",
    "    execute_query(conn_db, avst_fas_query)\n",
    "    print(f\"avst_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 4: Calculate the horizontal distance between tree and phase.\n",
    "    execute_query(conn_db, avst_hori_query)\n",
    "    print(f\"avst_hori was calculated succesfully for {LG}_{line}\") \n",
    "\n",
    "try: \n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(lambda row: calculate_distances(row, conn_db), axis=1)\n",
    "    \n",
    "    print(\"Successfully calculated distances for all wires.\") #TODO printed on error, change this\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Error while working with PostgreSQL:\", e) \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fyll i Littera, Ursprung, Ursprung_Datum, Registreringsdatum\n",
    "Borde göras i ett tidigare steg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data from LEDNIGNAR.txt\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "import arcpy\n",
    "\n",
    "SKB_location = os.path.join(working_gdb, 'SKB_XYZ')\n",
    "#print(SKB_location)\n",
    "\n",
    "date_field = 'RGDTM'\n",
    "#matosakerhet_h_field = 'MATOSAKERHET_HOJD'\n",
    "\n",
    "urspr = \"SWECO\"\n",
    "#ins_met = 20\n",
    "#matosak_plan = 1000\n",
    "#matosak_hojd = 1000\n",
    "lev_dat = str(date.today())\n",
    "\n",
    "def add_date_and_accuracy(powerline):\n",
    "    LG = powerline[\"LG\"]\n",
    "    line = powerline[\"line\"]\n",
    "    littera = powerline[\"Littera\"]\n",
    "    #LG_code = cvd_LEDNINGSGATA[LG]\n",
    "    regdatum = skanningsdatum[f\"{LG}_{line}\"]\n",
    "    \n",
    "    LG_line = f\"{LG}_{line}\"\n",
    "    print(f\"Doing {LG_line}\")\n",
    "\n",
    "    fc_SKB = f\"{LG}_{line}_SKB_XYZ\"\n",
    "    SKB_path = os.path.join(SKB_location, fc_SKB)\n",
    "    # Activating workspace\n",
    "    arcpy.env.workspace = SKB_path\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    \n",
    "\n",
    "    \n",
    "    arcpy.management.CalculateField(fc_SKB, \"LITTERA\", \"\".join((\"'\", littera, \"'\")))\n",
    "    arcpy.management.CalculateField(fc_SKB, \"Ursprung\", \"\".join((\"'\", urspr, \"'\")))\n",
    "    arcpy.management.CalculateField(fc_SKB, \"Ursprung_Datum\", \"\".join((\"'\", lev_dat, \"'\")))\n",
    "    arcpy.management.CalculateField(fc_SKB, \"RGDTM\", \"\".join((\"'\", regdatum, \"'\")))\n",
    "    #sr = arcpy.SpatialReference(\"SWEREF99_TM\")\n",
    "    #arcpy.DefineProjection_management(fc_SKB, sr)\n",
    "    # Assigning domains to fields\n",
    "    arcpy.management.AssignDomainToField(\n",
    "        fc_SKB, \"LITTERA\", \"cvd_LITTERA_LEDNING\")\n",
    "    \n",
    "    #print(\"Klart\")\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "\n",
    "df_skanningsdatum = pd.read_csv(scandate_file, sep=\"\\t\", header=0)\n",
    "skanningsdatum = {f\"{row['LG']}_{row['line']}\": row['skanningsdatum'] for _, row in df_skanningsdatum.iterrows()}\n",
    "\n",
    "#df_cvd_LEDNINGSGATA = pd.read_csv(cvd_LEDNINGSGATA_path, sep=\"\\t\", header=0)\n",
    "#df_cvd_DATE = pd.read_csv(cvd_DATE_path, sep=\"\\t\", header=0)\n",
    "#cvd_LEDNINGSGATA = {df_cvd_LEDNINGSGATA.LG[i]: df_cvd_LEDNINGSGATA.Code[i] for i in range(\n",
    "#    len(df_cvd_LEDNINGSGATA))}\n",
    "#cvd_DATE = {df_cvd_DATE.LG[i].join(\n",
    "#    \"_00\" + str(df_cvd_DATE.nr[i])): df_cvd_DATE.Datum[i] for i in range(len(df_cvd_DATE))}\n",
    "\n",
    "powerlines_df.apply(add_date_and_accuracy, axis=1)\n",
    "print(\"körning av cell klar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slå ihop kantträd för alla ledningar till en featureclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working_gdb = \"C:\\SVK_2024\\python_work_dir\\working_250218.gdb\"\n",
    "print(working_gdb)\n",
    "#print(Path(working_gdb,\"SKB_XYZ\"))\n",
    "arcpy.env.workspace = working_gdb\n",
    "#arcpy.env.workspace = os.path.join(working_gdb,\"SKB_XYZ\")\n",
    "print(arcpy.env.workspace)\n",
    "feature_classes = arcpy.ListFeatureClasses(feature_dataset=\"SKB_XYZ\")\n",
    "print(feature_classes)\n",
    "output_feature_classes = os.path.join(working_gdb,\"kantträd_oklippta\") #\"c:\\workspace\\working_25.gdb\\merged_feature_class\"\n",
    "print(output_feature_classes)\n",
    "arcpy.Merge_management(feature_classes, output_feature_classes)\n",
    "print(\"körning av cell klar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klipp träffpunkter\n",
    "Se till att använda aktuella data i LG_polygons och station_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#fc_ledningsgator = r\"Q:\\Projekt\\Data_2024\\Underlag_SVK\\Bestallningsunderlag_2024.gdb\\GNG_LEDNINGSGATA\" #os.path.join(gdb, \"Gator2021\")\n",
    "#fc_stationsomraden = r\"Q:\\Projekt\\Data_2024\\Underlag_SVK\\Bestallningsunderlag_2024.gdb\\GNG_STATIONSOMRADE\" #os.path.join(gdb, \"STATIONSOMRADE\")\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "##fd_ej_akuta = os.path.join(gdb, \"ej_akuta\")\n",
    "#fd_all = os.path.join(gdb, \"all\")\n",
    "#fd_traffpunkter = os.path.join(gdb, \"traffpunkter\")\n",
    "\n",
    "#fc_all = os.path.join(fd_all, f\"all_{version}\")\n",
    "#fc_traffpunkter = os.path.join(fd_traffpunkter, f\"traffpunkter_{version}\")\n",
    "tmp_layer_traffpunkter = os.path.join(working_gdb, \"tmp_layer_traffpunkter\")\n",
    "print(f\"tmp_layer_traffpunkter: {tmp_layer_traffpunkter}\")\n",
    "\n",
    "fc_traffpunkter = os.path.join(results_gdb, 'kantträd')\n",
    "print(f\"fc_traffpunkter: {fc_traffpunkter}\")\n",
    "\n",
    "fc_all = os.path.join(working_gdb, \"kantträd_oklippta\")\n",
    "print(f\"fc_all: {fc_all}\")\n",
    "\n",
    "#arcpy.management.Delete(tmp_layer_traffpunkter) #behövs detta bara i fall man kör om funktionen och tmp_layer_traffpunkter existerar \n",
    "\n",
    "\n",
    "\n",
    "# Gör kopia av oklippta\n",
    "arcpy.management.CopyFeatures(fc_all, fc_traffpunkter)\n",
    "\n",
    "arcpy.management.MakeFeatureLayer(fc_traffpunkter, tmp_layer_traffpunkter)\n",
    "\n",
    "# Steg 1\n",
    "# Inom fastbredd och stationsområden: ta bort alla inom 7 m horisontellt avstånd från fas\n",
    "#\n",
    "# 1.1 Välj alla inom fastbredd\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = LG_polygons,\n",
    "                                       selection_type = \"NEW_SELECTION\")\n",
    "# 1.2 Lägg till alla inom stationsområden\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = station_polygons,\n",
    "                                       selection_type = \"ADD_TO_SELECTION\")\n",
    "# 1.3 Av de valda, behåll punkter med avst_hori < 7 m\n",
    "arcpy.management.SelectLayerByAttribute(in_layer_or_view = tmp_layer_traffpunkter,\n",
    "                                        selection_type = \"SUBSET_SELECTION\", \n",
    "                                        where_clause = '\"AVSTAND_HORISONTELLT\" < 7')\n",
    "# 1.4 Radera valda\n",
    "arcpy.management.DeleteFeatures(tmp_layer_traffpunkter)\n",
    "\n",
    "# Steg 2\n",
    "# Inom fastbredd och stationsområden: ta bort alla med trädhöjd <= 4.5 m och avst_fas >= 5 m\n",
    "#\n",
    "# 2.1 Välj alla inom fastbredd\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = LG_polygons,\n",
    "                                       selection_type = \"NEW_SELECTION\")\n",
    "# 2.2 Lägg till alla inom stationsområden\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = station_polygons,\n",
    "                                       selection_type = \"ADD_TO_SELECTION\")\n",
    "# 2.3 Av de valda, gör gör subset selection med punkter med trädhöjd <= 4.5 OCH avst_fas >= 5\n",
    "arcpy.management.SelectLayerByAttribute(in_layer_or_view = tmp_layer_traffpunkter,\n",
    "                                       selection_type = \"SUBSET_SELECTION\",\n",
    "                                       where_clause = '\"DELTA_HOJD\" <= 4.5 AND \"AVSTAND_FAS\" >= 5')\n",
    "# 2.4 Radera valda\n",
    "arcpy.management.DeleteFeatures(tmp_layer_traffpunkter)\n",
    "\n",
    "## Kopiera kvarvarande punkter till traffpunkter\n",
    "#arcpy.management.CopyFeatures(tmp_layer_traffpunkter, fc_traffpunkter) # Överflödigt?\n",
    "\n",
    "# Ta bort det temporära lagret\n",
    "arcpy.management.Delete(tmp_layer_traffpunkter)\n",
    "\n",
    "print(\"körning av cell klar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "import arcpy\n",
    "\n",
    "#r\"C:\\SVK_2024\\python_work_dir\\results_250218.gdb\"\n",
    "#results_gdb = os.path.join(local_dir, f\"results_{run_ID}.gdb\")\n",
    "#print(Path(results_gdb))\n",
    "\n",
    "arcpy.env.workspace = working_gdb\n",
    "print(arcpy.env.workspace)\n",
    "kanttrad = os.path.join(results_gdb, 'kantträd')\n",
    "print(kanttrad)\n",
    "\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"LEDNINGSGATA\", \"cvd_LEDNINGSGATA\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"LITTERA\", \"cvd_LITTERA_LEDNING\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Matosakerhet_Hojd\", \"cvd_MATOSAKERHET\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Matosakerhet_Plan\", \"cvd_MATOSAKERHET\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Insamlingsmetod\", \"cvd_INMATNINGSMETOD\")\n",
    "\n",
    "\n",
    "\n",
    "#print(kanttrad)\n",
    "#arcpy.env.workspace = kanttrad\n",
    "#arcpy.env.workspace(kanttrad)\n",
    "#print(arcpy.env.workspace)\n",
    "#feature_classes = arcpy.ListFeatureClasses(feature_dataset=\"SKB_XYZ\")\n",
    "print(\"körning av cell klar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kontroll\n",
    "### Här kan man lägga in egna skript för kontroll av resultatet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLytta till RBX-notebook.\n",
    "# Listar alla LG - littera som finns i RBX-resultatet\n",
    "# Jämför detta med logg-filen för att se att rätt ledningar\n",
    "# är med i resultatet (inte alla ledningar har RBX-punkter)\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "gdb_path = r\"C:\\SVK_2024\\pythonkörningar\\results_2501xx.gdb\"\n",
    "feature_class = \"RBX_closest_points\"\n",
    "\n",
    "gdf = gpd.read_file(gdb_path, layer=feature_class)\n",
    "\n",
    "gdf = gdf[[\"LEDNINGSGATA\", \"LITTERA\"]]\n",
    "unique_df = gdf.drop_duplicates().sort_values([\"LEDNINGSGATA\", \"LITTERA\"])\n",
    "\n",
    "print(unique_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
