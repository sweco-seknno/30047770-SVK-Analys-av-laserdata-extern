{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Trees Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEKNNO.json\"\n",
    "#settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEVPLI.json\"\n",
    "#settings_file = r\"C:\\SVK_utveckling\\settings_SEVPLI.json\"\n",
    "settings_file = r\"C:\\Users\\SE1K4H\\Desktop\\SVK-Analys-Filer\\settings_test_PostGIS.json\" # Working on Elsas's computer\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(settings_file):\n",
    "        raise FileNotFoundError(f\"Could not find {settings_file}\")\n",
    "\n",
    "    with open(settings_file, 'r', encoding='utf-8') as file:\n",
    "        settings = json.load(file)\n",
    "\n",
    "    # TODO: check which ones are needed and not\n",
    "    run_ID = settings[\"run_ID\"]\n",
    "    powerline_list = settings[\"powerline_list\"]\n",
    "    local_dir = settings[\"local_folder\"]\n",
    "    results_gdb_template = settings[\"results_gdb_template\"]\n",
    "    domains_folder = settings[\"domains_folder\"]\n",
    "    scandate_file = settings[\"scandate_file\"]\n",
    "    cvd_LEDNINGSGATA_path = os.path.join(domains_folder, \"cvd_LEDNINGSGATA.txt\")\n",
    "    powerlines_folder = settings[\"powerlines_folder\"]\n",
    "    LG_polygons = settings[\"LG_polygons\"]\n",
    "    station_polygons = settings[\"station_polygons\"]\n",
    "    order_gdb = settings[\"order\"]\n",
    "    ogr2ogr_path = settings[\"ogr2ogr_path\"]   \n",
    "    proj_lib_path = settings[\"proj_lib_path\"]\n",
    "    gdal_data_path = settings[\"gdal_data_path\"]\n",
    "    DEFAULT_DB_NAME = settings[\"default_db_name\"]\n",
    "    DB_NAME = settings[\"db_name\"]\n",
    "    USER = settings[\"db_user\"]\n",
    "    PASSWORD = settings[\"db_password\"]\n",
    "    HOST = settings[\"host\"]\n",
    "    PORT = settings[\"port\"] \n",
    "\n",
    "    if None in (run_ID, powerline_list, local_dir, results_gdb_template, domains_folder, scandate_file, powerlines_folder, LG_polygons, station_polygons, order_gdb, ogr2ogr_path, proj_lib_path, gdal_data_path, DEFAULT_DB_NAME, DB_NAME, USER, PASSWORD, HOST, PORT):\n",
    "        raise KeyError(f\"One or more keys are missing in {settings_file}\")\n",
    "    \n",
    "    results_gdb = os.path.join(local_dir, f\"results_{run_ID}_PostGIS.gdb\")\n",
    "    \n",
    "    print(f\"Settings loaded successfully.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Invalid JSON format in {settings_file}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PostGIS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "\n",
    "# TODO can move these kind of methods to utils or similiar?\n",
    "def connect_to_database(db_name):\n",
    "    try:\n",
    "        conn = psycopg.connect(dbname=db_name, user=USER, password=PASSWORD, host=HOST, port=PORT)\n",
    "        conn.autocommit = True\n",
    "        return conn\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"Error connecting to {db_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def execute_query(conn, query, data=None, fetch=False):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            #print(f\"Running query: {query.as_string(conn)}\") # Add this line for debugging purpose\n",
    "            cur.execute(query, data or ())\n",
    "            if fetch:\n",
    "                return cur.fetchall()\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy template gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(results_gdb):\n",
    "    print(f\"{results_gdb} already exists\")\n",
    "else: \n",
    "    shutil.copytree(results_gdb_template, results_gdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge SKB text files for all blocks of a powerline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import fileinput\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# TODO add exception handling.\n",
    "\n",
    "def combine_blocks(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    \n",
    "    block_dir = os.path.join(line_dir, \"kantträd\", \"block\")\n",
    "    combined_blocks_path = os.path.join(line_dir, \"kantträd\", \"SKB_raw.txt\")\n",
    "    \n",
    "    # Merge files for all blocks into one\n",
    "    merge_blocks(block_dir, \"*.txt\", combined_blocks_path)\n",
    "    print(f\"Successfully merged all edge trees blocks for {LG}/line_{line} into text file {combined_blocks_path}.\")\n",
    "\n",
    "def merge_blocks(src_dir, search_pattern, dst_file):\n",
    "    blocks = glob.glob(os.path.join(src_dir, search_pattern))\n",
    "    with open(dst_file, \"w\") as fh:\n",
    "        input_lines = fileinput.input(blocks)\n",
    "        fh.writelines(input_lines)\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "powerlines_df.apply(combine_blocks, axis=1)\n",
    "print(f\"Done with all power lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Edge Trees to PostGIS Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from psycopg import sql #TODO update to and install psycopg, the latest version.\n",
    "\n",
    "cur = None\n",
    "conn = None\n",
    "\n",
    "def create_SKB_XYmZ_table(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    SKB_dir = line_dir / \"kantträd\"\n",
    "    SKB_file_in = os.path.join(SKB_dir, f\"SKB_raw.txt\")\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name))\n",
    "    create_query = sql.SQL(\"CREATE TABLE {table_name}(objectid SERIAL PRIMARY KEY, x DOUBLE PRECISION, y DOUBLE PRECISION, z FLOAT, dz FLOAT, mz FLOAT, shape geometry(POINTZ, 3006))\").format(table_name=sql.Identifier(table_name)) #TODO potentially change to 5845\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create table for powerline\n",
    "        execute_query(conn_db, drop_table_if_exists_query)\n",
    "        execute_query(conn_db, create_query)\n",
    "        \n",
    "        # Step 2: Insert edge trees into powerline table \n",
    "        with open(SKB_file_in, 'r') as src_file:\n",
    "            for file_line in src_file:\n",
    "                l_split = file_line.split(' ')\n",
    "                x = float(l_split[0])\n",
    "                y = float(l_split[1])\n",
    "                z = float(l_split[2])\n",
    "                dz = float(l_split[3])\n",
    "                mz = z - dz\n",
    "                \n",
    "                #TODO potentially change to 5845, also check if SRID is needed on table level as well\n",
    "                # TODO check if I should change to ST_SetSRID(ST_MakePoint(x, y, z), 3006)\n",
    "                insert_query = sql.SQL(\"\"\"\n",
    "                    INSERT INTO {table_name} (x, y, z, dz, mz, shape)\n",
    "                    VALUES (%s, %s, %s, %s, %s, ST_GeomFromText(%s, 3006))\n",
    "                \"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "                pointz_string = f\"POINTZ({x} {y} {mz})\"\n",
    "                tree_data = (x, y, z, dz, mz, pointz_string)\n",
    "                execute_query(conn_db, insert_query, tree_data)\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(create_SKB_XYmZ_table, axis=1)\n",
    "    \n",
    "    print(\"Successfully inserted edge trees to PostGIS database.\") \n",
    "\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_distances(row, conn_db):\n",
    "    #TODO se över SQL query om de kan förbättras, optimeras\n",
    "    #TODO Borde ta bort columner först om de existerar \n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    wire_table_name = f\"{LG.lower()}_{line}_fas\"\n",
    "    \n",
    "    add_columns_query = sql.SQL(\"\"\"ALTER TABLE {table_name}\n",
    "        ADD COLUMN IF NOT EXISTS avst_mz_fas DOUBLE PRECISION,\n",
    "        ADD COLUMN IF NOT EXISTS avst_hori DOUBLE PRECISION, \n",
    "        ADD COLUMN IF NOT EXISTS avst_fas DOUBLE PRECISION;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "    \n",
    "    avst_mz_fas_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_mz_fas = subquery.distance\n",
    "        FROM (\n",
    "            SELECT DISTINCT ON (p.objectid)  \n",
    "                p.objectid,\n",
    "                ST_3DDistance(p.shape, l.shape) AS distance\n",
    "            FROM \n",
    "                {table_name} p, \n",
    "                {wire_table_name} l\n",
    "            WHERE \n",
    "                ST_3DDWithin(p.shape, l.shape, 100)\n",
    "            ORDER BY \n",
    "                p.objectid, distance ASC\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name), wire_table_name=sql.Identifier(wire_table_name))\n",
    "\n",
    "    avst_fas_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_fas = subquery.distance\n",
    "        FROM (\n",
    "            SELECT objectid, avst_mz_fas - dz AS distance\n",
    "            FROM {table_name}\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    avst_hori_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_hori = subquery.distance\n",
    "        FROM (\n",
    "            SELECT DISTINCT ON (p.objectid)  \n",
    "                p.objectid,\n",
    "                ST_Distance(p.shape, l.shape) AS distance\n",
    "            FROM \n",
    "                {table_name} p, \n",
    "                {wire_table_name} l\n",
    "            WHERE \n",
    "                ST_DWithin(p.shape, l.shape, 100)\n",
    "            ORDER BY \n",
    "                p.objectid, distance ASC\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name), wire_table_name=sql.Identifier(wire_table_name))\n",
    "\n",
    "    # Step 1: Add attributes\n",
    "    execute_query(conn_db, add_columns_query)\n",
    "    print(f\"Columns were added successfully to table {table_name}\")\n",
    "\n",
    "    # Step 2: Calculate shortest distance between mz and phase.\n",
    "    execute_query(conn_db, avst_mz_fas_query)\n",
    "    print(f\"avst_mz_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 3: Calculate the shortest distance between tree top (z) and phase with event of potential fall.\n",
    "    execute_query(conn_db, avst_fas_query)\n",
    "    print(f\"avst_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 4: Calculate the horizontal distance between tree and phase.\n",
    "    execute_query(conn_db, avst_hori_query)\n",
    "    print(f\"avst_hori was calculated succesfully for {LG}_{line}\") \n",
    "\n",
    "try: \n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(lambda row: calculate_distances(row, conn_db), axis=1)\n",
    "    \n",
    "    print(\"Successfully calculated distances for all wires.\") #TODO printed on error, change this\n",
    "except psycopg.Error as e:\n",
    "    print(\"Error while working with PostgreSQL:\", e) \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Littera, Ursprung, Ursprung_Datum, Registreringsdatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "import pandas as pd\n",
    "from psycopg import sql\n",
    "from datetime import date\n",
    "\n",
    "def add_data(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    littera = row[\"Littera\"]\n",
    "    regdatum = skanningsdatum[f\"{LG}_{line}.0\"]   \n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "\n",
    "    #TODO ledningsgata, insamlingsmetod, matosakerhet x2 should be updated to domains, using foreign keys \n",
    "    add_columns_query = sql.SQL(\"\"\"ALTER TABLE {table_name}\n",
    "                                    ADD COLUMN IF NOT EXISTS rgdtm DATE, \n",
    "                                    ADD COLUMN IF NOT EXISTS ledningsgata FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS insamlingsmetod FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS matosakerhet_plan FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS matosakerhet_hojd FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS littera VARCHAR(255),\n",
    "                                    ADD COLUMN IF NOT EXISTS ursprung VARCHAR(255),\n",
    "                                    ADD COLUMN IF NOT EXISTS ursprung_datum DATE;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    # TODO Update littera to work for domain\n",
    "    # TODO Investigate if % or placeholder should be used\n",
    "    update_query = sql.SQL(\"\"\"UPDATE {table_name}\n",
    "                                SET rgdtm = {regdatum}, \n",
    "                                littera = {littera},\n",
    "                                ursprung = {ursprung},\n",
    "                                ursprung_datum = {lev_dat};\"\"\").format(table_name=sql.Identifier(table_name), \n",
    "                                                                       regdatum=sql.Placeholder(\"regdatum\"), \n",
    "                                                                       littera=sql.Placeholder(\"littera\"), \n",
    "                                                                       ursprung=sql.Placeholder(\"ursprung\"), \n",
    "                                                                       lev_dat=sql.Placeholder(\"lev_dat\"))\n",
    "    updated_values = {\"regdatum\": regdatum, \"littera\": littera, \"ursprung\": urspr, \"lev_dat\": lev_dat}\n",
    "\n",
    "    try:\n",
    "        # Step 1: Add columns\n",
    "        execute_query(conn_db, add_columns_query)\n",
    "        print(f\"Columns were added successfully to table {table_name}\")\n",
    "        \n",
    "        # Step 2: Update Littera, Ursprung, Ursprung_Datum, Registreringsdatum \n",
    "        execute_query(conn_db, update_query, updated_values)\n",
    "        print(f\"Values were updated successfully in table {table_name}\")\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    df_skanningsdatum = pd.read_csv(scandate_file, sep=\"\\t\", header=0) #TODO english variables?\n",
    "    skanningsdatum = {f\"{row['LG']}_{row['line']}\": row['skanningsdatum'] for _, row in df_skanningsdatum.iterrows()}\n",
    "    urspr = \"SWECO\"\n",
    "    lev_dat = str(date.today())\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(add_data, axis=1) #TODO don't have to pass conn_db on the other places, do like this instead\n",
    "    \n",
    "    print(\"Successfully added columns and updated values.\") \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge edge trees for all powerlines into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_skb_xymz_tables():\n",
    "    select_tables_query = sql.SQL(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name LIKE '%%skb_xymz' AND table_schema = 'public';\n",
    "    \"\"\")\n",
    "\n",
    "    result = execute_query(conn_db, select_tables_query, fetch=True)\n",
    "    table_names = [t[0] for t in result]\n",
    "\n",
    "    return table_names\n",
    "\n",
    "def merge_tables(table_names):\n",
    "    table_name = 'kantträd_oklippta'\n",
    "\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name)) #TODO maybe these two queries can be combined into one?\n",
    "\n",
    "    create_table_query = sql.SQL(\"\"\"\n",
    "        CREATE TABLE {table_name} AS\n",
    "        TABLE {first_table} WITH NO DATA;\n",
    "    \"\"\").format(table_name=sql.Identifier(table_name), first_table=sql.Identifier(table_names[0]))\n",
    "\n",
    "    union_parts = []\n",
    "    for table in table_names:\n",
    "        part = sql.SQL(\"\"\"\n",
    "            SELECT * FROM (\n",
    "                SELECT *\n",
    "                FROM {table}\n",
    "                ORDER BY objectid\n",
    "            )\n",
    "        \"\"\").format(table=sql.Identifier(table))\n",
    "        union_parts.append(part)\n",
    "\n",
    "    union_query = sql.SQL(\" UNION ALL \").join(union_parts)\n",
    "\n",
    "    insert_query = sql.SQL(\"\"\"\n",
    "        INSERT INTO {table_name} (objectid, x, y, z, dz, mz, shape, avst_mz_fas, avst_hori, avst_fas, rgdtm, ledningsgata, insamlingsmetod, matosakerhet_plan, matosakerhet_hojd, littera, ursprung, ursprung_datum)\n",
    "        SELECT\n",
    "            ROW_NUMBER() OVER () AS objectid, x, y, z, dz, mz, shape, avst_mz_fas, avst_hori, avst_fas, rgdtm, ledningsgata, insamlingsmetod, matosakerhet_plan, matosakerhet_hojd, littera, ursprung, ursprung_datum\n",
    "        FROM (\n",
    "            {unioned}\n",
    "        )\n",
    "    \"\"\").format(table_name=sql.Identifier(table_name), unioned=union_query)\n",
    "\n",
    "    execute_query(conn_db, drop_table_if_exists_query)\n",
    "    execute_query(conn_db, create_table_query)\n",
    "    execute_query(conn_db, insert_query)\n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        table_names = list_skb_xymz_tables()        \n",
    "        if table_names:\n",
    "            table_names = sorted(table_names)\n",
    "            print(f\"Found tables: {table_names}\")\n",
    "            merge_tables(table_names)\n",
    "            print(\"Successfully merged all edge trees tables into one table.\") #TODO This should not be printed if sql error occurs..\n",
    "        else:\n",
    "            print(\"No tables ending with 'skb_xymz' found.\")\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Intersection Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from psycopg import sql\n",
    "\n",
    "#TODO in general in all cells, take a look att exception handling\n",
    "PG_CONNECTION = f\"PG:host={HOST} dbname={DB_NAME} user={USER} password={PASSWORD} port={PORT}\"\n",
    "\n",
    "def convert_polygon_fc_to_postGIS(table_name):\n",
    "    #TODO no vertical system was added, look more into that.\n",
    "    ogr_command = [\n",
    "        ogr2ogr_path,\n",
    "        \"-f\", \"PostgreSQL\",\n",
    "        PG_CONNECTION,\n",
    "        order_gdb,\n",
    "        \"-a_srs\", \"EPSG:3006\", # TODO check if RH2000 should be specified?\n",
    "        \"-nln\", table_name,\n",
    "        \"-nlt\", \"MULTIPOLYGONZ\", #TODO Check if this was the correct choice\n",
    "        \"-dim\", \"3\",\n",
    "        \"-overwrite\",    \n",
    "        table_name\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        os.environ[\"PROJ_LIB\"] = proj_lib_path #TODO potentially solve this in another way. Now it has to be done in order to find -a_srs EPSG:3006\n",
    "        os.environ[\"GDAL_DATA\"] = gdal_data_path\n",
    "        subprocess.run(ogr_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully converted {order_gdb} to PostGIS as '{table_name}'.\") #TODO verify this\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: Could not convert {order_gdb} to PostGIS. {e}\")\n",
    "\n",
    "def copy_table(new_table, copied_table):\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {new_table}\").format(new_table=sql.Identifier(new_table)) #TODO maybe these two queries can be combined into one?\n",
    "\n",
    "    copy_table_query = sql.SQL(\"\"\"\n",
    "        CREATE TABLE {new_table} AS\n",
    "        SELECT *\n",
    "        FROM {copied_table};\n",
    "    \"\"\").format(new_table=sql.Identifier(new_table), copied_table=sql.Identifier(copied_table))\n",
    "\n",
    "    execute_query(conn_db, drop_table_if_exists_query)\n",
    "    execute_query(conn_db, copy_table_query)\n",
    "  \n",
    "def extract_trees(table_name, where_clause):\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    avst_hori_query = sql.SQL(\"\"\"\n",
    "        CREATE TABLE {table} AS\n",
    "\n",
    "        SELECT oklippta.*\n",
    "        FROM kantträd_oklippta oklippta\n",
    "        JOIN gng_ledningsgata lg\n",
    "        ON ST_Intersects(oklippta.shape, lg.shape)\n",
    "        WHERE {where_clause}\n",
    "\n",
    "        UNION \n",
    "\n",
    "        SELECT oklippta.*\n",
    "        FROM kantträd_oklippta oklippta\n",
    "        JOIN gng_stationsomrade station\n",
    "        ON ST_Intersects(oklippta.shape, station.shape)\n",
    "        WHERE {where_clause}\n",
    "    \"\"\").format(table=sql.Identifier(table_name), where_clause=sql.SQL(where_clause)) # TODO structure where_clause in another way?\n",
    "\n",
    "    execute_query(conn_db, drop_table_if_exists_query)\n",
    "    execute_query(conn_db, avst_hori_query)\n",
    "\n",
    "def clip_trees(table_1, table_2):\n",
    "    delete_from_query = sql.SQL(\"\"\"\n",
    "        DELETE FROM kantträd\n",
    "        WHERE EXISTS (\n",
    "            SELECT 1\n",
    "            FROM {table_1} AS t1\n",
    "            WHERE t1.objectid = kantträd.objectid\n",
    "        ) \n",
    "\n",
    "        OR EXISTS (\n",
    "            SELECT 1\n",
    "            FROM {table_2} AS t2\n",
    "            WHERE t2.objectid = kantträd.objectid\n",
    "        );\n",
    "    \"\"\").format(table_1=sql.Identifier(table_1), table_2=sql.Identifier(table_2))\n",
    "\n",
    "    execute_query(conn_db, delete_from_query)\n",
    "\n",
    "try:   \n",
    "    table_1 = \"tmp_traffpunkter_1\"\n",
    "    table_2 = \"tmp_traffpunkter_2\"\n",
    "\n",
    "    convert_polygon_fc_to_postGIS('gng_ledningsgata')\n",
    "    convert_polygon_fc_to_postGIS('gng_stationsomrade')\n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        copy_table(\"kantträd\", \"kantträd_oklippta\")\n",
    "        # Extract trees within the powerline corridor and the station area that are within 7 meters from phase.\n",
    "        extract_trees(table_1, \"avst_hori < 7\")\n",
    "        # Extract trees within the powerline corridor and the station area with tree height <= 4.5 meters and distance to phase >= 5 meters.\n",
    "        extract_trees(table_2, \"dz <= 4.5 AND avst_fas >= 5\")\n",
    "        clip_trees(table_1, table_2)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert edge tree PostGIS table to file geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Rename SKB_XYmZ feature class, dataset and table from the start to SKB_XYZ\n",
    "# TODO Delete fields like x, y, z\n",
    "# TODO check if shape should be mz or z, currently mz? \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "def postGIS_to_gdb():\n",
    "    table_name = \"kantträd\"\n",
    "\n",
    "    ogr_command = [\n",
    "        ogr2ogr_path,\n",
    "        \"-f\", \"OpenFileGDB\",\n",
    "        results_gdb,      \n",
    "        pg_connection,\n",
    "        \"-a_srs\", \"EPSG:3006\",\n",
    "        \"-nlt\", \"POINTZ\",\n",
    "        \"-dim\", \"3\",\n",
    "        \"-overwrite\",\n",
    "        \"-sql\", f\"SELECT * FROM public.{table_name}\",\n",
    "        \"-nln\", table_name,\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ogr_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully converted PostGIS {table_name} into gdb {results_gdb}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during import: {e}\")\n",
    "\n",
    "try:\n",
    "    pg_connection = f\"PG:host={HOST} dbname={DB_NAME} user={USER} password={PASSWORD} port={PORT}\"\n",
    "    ogr2ogr_path = ogr2ogr_path\n",
    "    os.environ[\"PROJ_LIB\"] = proj_lib_path #TODO potentially solve this in another way, but needed to specify EPSG:3006\n",
    "    os.environ[\"GDAL_DATA\"] = gdal_data_path\n",
    "\n",
    "    postGIS_to_gdb()\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "import arcpy\n",
    "\n",
    "#r\"C:\\SVK_2024\\python_work_dir\\results_250218.gdb\"\n",
    "#results_gdb = os.path.join(local_dir, f\"results_{run_ID}.gdb\")\n",
    "#print(Path(results_gdb))\n",
    "\n",
    "# arcpy.env.workspace = working_gdb\n",
    "print(arcpy.env.workspace)\n",
    "kanttrad = os.path.join(results_gdb, 'kantträd')\n",
    "print(kanttrad)\n",
    "\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"LEDNINGSGATA\", \"cvd_LEDNINGSGATA\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"LITTERA\", \"cvd_LITTERA_LEDNING\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Matosakerhet_Hojd\", \"cvd_MATOSAKERHET\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Matosakerhet_Plan\", \"cvd_MATOSAKERHET\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Insamlingsmetod\", \"cvd_INMATNINGSMETOD\")\n",
    "\n",
    "\n",
    "\n",
    "#print(kanttrad)\n",
    "#arcpy.env.workspace = kanttrad\n",
    "#arcpy.env.workspace(kanttrad)\n",
    "#print(arcpy.env.workspace)\n",
    "#feature_classes = arcpy.ListFeatureClasses(feature_dataset=\"SKB_XYZ\")\n",
    "print(\"körning av cell klar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kontroll\n",
    "### Här kan man lägga in egna skript för kontroll av resultatet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLytta till RBX-notebook.\n",
    "# Listar alla LG - littera som finns i RBX-resultatet\n",
    "# Jämför detta med logg-filen för att se att rätt ledningar\n",
    "# är med i resultatet (inte alla ledningar har RBX-punkter)\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "gdb_path = r\"C:\\SVK_2024\\pythonkörningar\\results_2501xx.gdb\"\n",
    "feature_class = \"RBX_closest_points\"\n",
    "\n",
    "gdf = gpd.read_file(gdb_path, layer=feature_class)\n",
    "\n",
    "gdf = gdf[[\"LEDNINGSGATA\", \"LITTERA\"]]\n",
    "unique_df = gdf.drop_duplicates().sort_values([\"LEDNINGSGATA\", \"LITTERA\"])\n",
    "\n",
    "print(unique_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
