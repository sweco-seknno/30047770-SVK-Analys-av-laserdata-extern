{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_ID': '250128', 'powerline_list': 'Q:/Projekt/Data_2024/styrfiler/veg_kanttrad_250128.txt', 'scandate_file': 'Q:/Projekt/Data_2024/skanningsdatum/skanningsdatum_250128.txt', 'local_folder': 'C:/SVK_2024/pythonkörningar', 'working_gdb_template': 'Q:/Projekt/Data_2024/working_template.gdb', 'results_gdb_template': 'Q:/Projekt/Data_2024/results_template.gdb', 'wires_gdb_template': 'Q:/Projekt/Data_2024/wires_template.gdb', 'dgn_folder': 'Q:/Projekt/Analys_2024/DGN_2024', 'powerlines_folder': 'Q:/Projekt/Analys_2024/ledningar', 'domains_folder': 'Q:/Projekt/Data_2024/Underlag_SVK/Domains', 'LG_polygons': 'Q:/Projekt/Data_2024/Underlag_SVK/Bestallningsunderlag_2024.gdb/GNG_LEDNINGSGATA', 'station_polygons': 'Q:/Projekt/Data_2024/Underlag_SVK/Bestallningsunderlag_2024.gdb/GNG_STATIONSOMRADE'}\n"
     ]
    }
   ],
   "source": [
    "# Läs settings\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pyproj\n",
    "\n",
<<<<<<< Updated upstream
    "settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEKNNO.json\"\n",
=======
    "settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEKNNO_postgis.json\"\n",
>>>>>>> Stashed changes
    "\n",
    "with open(settings_file, 'r', encoding='utf-8') as file:\n",
    "    settings = json.load(file)\n",
    "\n",
    "print(settings)\n",
    "\n",
    "run_ID = settings[\"run_ID\"]\n",
    "powerline_list = settings[\"powerline_list\"]\n",
    "local_dir = settings[\"local_folder\"]\n",
    "powerlines_folder = settings[\"powerlines_folder\"]\n",
    "domains_folder = settings[\"domains_folder\"]\n",
    "scandate_file = settings[\"scandate_file\"]\n",
    "working_gdb_template = settings[\"working_gdb_template\"]\n",
    "results_gdb_template = settings[\"results_gdb_template\"]\n",
    "cvd_LEDNINGSGATA_path = os.path.join(domains_folder, \"cvd_LEDNINGSGATA.txt\")\n",
    "wires_gdb = os.path.join(local_dir, f\"wires_{run_ID}.gdb\")\n",
    "RBX_shape_folder = os.path.join(local_dir, f\"RBX_shp_{run_ID}\")\n",
    "working_gdb = os.path.join(local_dir, f\"working_{run_ID}.gdb\")\n",
    "results_gdb = os.path.join(local_dir, f\"results_{run_ID}.gdb\")\n",
    "LG_polygons = settings[\"LG_polygons\"]\n",
    "station_polygons = settings[\"station_polygons\"]\n",
    "\n",
    "sr = arcpy.SpatialReference(\"SWEREF99_TM\", \"RH2000\")"
=======
    "    if None in (run_ID, powerline_list, local_dir, results_gdb_template, scandate_file, powerlines_folder, LG_polygons, station_polygons, order_gdb, ogr2ogr_path, proj_lib_path, gdal_data_path, DEFAULT_DB_NAME, DB_NAME, USER, PASSWORD, HOST, PORT):\n",
    "        raise KeyError(f\"One or more keys are missing in {settings_file}\")\n",
    "    \n",
    "    results_gdb = os.path.join(local_dir, f\"results_{run_ID}.gdb\")\n",
    "\n",
    "    pyproj.datadir.set_data_dir(proj_lib_path)\n",
    "    \n",
    "    print(f\"Settings loaded successfully.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Invalid JSON format in {settings_file}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PostGIS methods"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as psycopg\n",
    "\n",
    "# TODO can move these kind of methods to utils or similiar?\n",
    "def connect_to_database(db_name):\n",
    "    try:\n",
    "        conn = psycopg.connect(dbname=db_name, user=USER, password=PASSWORD, host=HOST, port=PORT)\n",
    "        conn.autocommit = True\n",
    "        return conn\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"Error connecting to {db_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def execute_query(conn, query, data=None, fetch=False):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            #print(f\"Running query: {query.as_string(conn)}\") # Add this line for debugging purpose\n",
    "            cur.execute(query, data or ())\n",
    "            if fetch:\n",
    "                return cur.fetchall()\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy template gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "C:/SVK_2024/pythonkörningar\\RBX_shp_250128 finns redan\n",
      "C:/SVK_2024/pythonkörningar\\working_250128.gdb finns redan\n",
      "C:/SVK_2024/pythonkörningar\\results_250128.gdb finns redan\n"
=======
      "C:/Uppdrag/2024/SVK_2024/pythonkörningar\\results_250425.gdb already exists\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Create empty folder for shapefiles\n",
    "if os.path.exists(RBX_shape_folder):\n",
    "    print(f\"{RBX_shape_folder} finns redan\")\n",
    "else:\n",
    "    os.makedirs(RBX_shape_folder)\n",
    "\n",
    "# Create working gdb as copy of template\n",
    "if os.path.exists(working_gdb):\n",
    "    print(f\"{working_gdb} finns redan\")\n",
    "else: \n",
    "    shutil.copytree(working_gdb_template, working_gdb)\n",
    "\n",
    "# Create results gdb as copy of template\n",
    "if os.path.exists(results_gdb):\n",
    "    print(f\"{results_gdb} finns redan\")\n",
    "else: \n",
    "    shutil.copytree(results_gdb_template, results_gdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Funktioner"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Funktioner som används av SKB.ipynb\n",
    " \n",
    "import os\n",
    "import arcpy\n",
    "arcpy.CheckOutExtension('3D')\n",
    "\n",
    "\n",
    "def create_SKB_XYmZ(txt_file, gdb_name, fc_name, sr):\n",
    "\n",
    "    fc_attributes = [['X', 'DOUBLE'],\n",
    "                     ['Y', 'DOUBLE'],\n",
    "                     ['Z', 'FLOAT'],\n",
    "                     ['dZ', 'FLOAT'],\n",
    "                     ['mZ', 'FLOAT']]\n",
    "\n",
    "    fc_all_fields = ['X', 'Y', 'Z', 'dZ', 'mZ', 'SHAPE@XY', 'SHAPE@Z']\n",
    "\n",
    "    txt_file_fields = ['X', 'Y', 'Z', 'dZ']\n",
    "\n",
    "    arcpy.CreateFeatureclass_management(out_path=gdb_name, out_name=fc_name,\n",
    "                                        geometry_type='POINT', has_z='ENABLED',\n",
    "                                        spatial_reference=sr)\n",
    "\n",
    "    for attribute in fc_attributes:\n",
    "        arcpy.AddField_management(in_table=os.path.join(gdb_name, fc_name),\n",
    "                                  field_name=attribute[0], field_type=attribute[1])\n",
    "\n",
    "    # Read txt file, insert line by line in feature class\n",
    "    with open(txt_file, 'r') as srcFile:\n",
    "        fc = os.path.join(gdb_name, fc_name)\n",
    "        with arcpy.da.InsertCursor(fc, fc_all_fields) as i_cursor:\n",
    "            for fileLine in srcFile:\n",
    "                lSplit = fileLine.split(' ')\n",
    "                X = float(lSplit[0])\n",
    "                Y = float(lSplit[1])\n",
    "                Z = float(lSplit[2])\n",
    "                dZ = float(lSplit[3])\n",
    "                mZ = Z - dZ\n",
    "                shapeXYZ = [(X, Y), mZ]\n",
    "                row = [X, Y, Z, dZ, mZ] + shapeXYZ\n",
    "                i_cursor.insertRow(row)\n",
    "\n",
    "\n",
    "def create_SKB_XYZ(gdb, fc, fc_template, sr):\n",
    "    # function to create fc_XYZ based on delivery template\n",
    "    # only execute this if the fc does not already exist\n",
    "    fc_full_path = os.path.join(gdb, fc)\n",
    "    if arcpy.Exists(fc_full_path):\n",
    "        print(fc+' already exists. Not created')\n",
    "    else:\n",
    "        arcpy.CreateFeatureclass_management(out_path=gdb, out_name=fc,\n",
    "                                            geometry_type='POINT', template=fc_template,\n",
    "                                            has_z='SAME_AS_TEMPLATE', spatial_reference=sr)\n",
    "\n",
    "\n",
    "def dist_mZ_wire(gdb_name, fc_name, wires, radius):\n",
    "    # Compute distance from tree ground point to wire\n",
    "    # (Tree height can then be subtracted from this value to compute AVST_FAS)\n",
    "    fc = os.path.join(gdb_name, fc_name)\n",
    "    arcpy.Near3D_3d(in_features=fc, near_features=wires, search_radius=radius,\n",
    "                    location=\"NO_LOCATION\", angle=\"NO_ANGLE\", delta=\"NO_DELTA\")\n",
    "    print('done with Near3D '+fc_name)\n",
    "    arcpy.DeleteField_management(in_table=fc, drop_field='NEAR_FID')\n",
    "    arcpy.AlterField_management(in_table=fc, field='NEAR_DIST',\n",
    "                                new_field_name='AVSTAND_HORISONTELLT')\n",
    "    arcpy.AlterField_management(in_table=fc, field='NEAR_DIST3',\n",
    "                                new_field_name='AVST_MZ_FAS')\n",
    "    print('done with deletefield '+fc_name)\n",
    "\n",
    "\n",
    "def LG_name(LG):\n",
    "    if LG <= 9:\n",
    "        LG_str = 'LG00'+str(LG)\n",
    "    elif LG <= 99:\n",
    "        LG_str = 'LG0'+str(LG)\n",
    "    else:\n",
    "        LG_str = 'LG'+str(LG)\n",
    "\n",
    "    return (LG_str)\n",
    "\n",
    "\n",
    "def pick_out_akuta_and_traffpunkter(gdb, fc_in, fc_akuta, fc_trfpt, fc_intr_ers):\n",
    "    # function for selecting\n",
    "    # 1) akuta trad, and\n",
    "    # 2) traffpunkter\n",
    "    # and save in separate feature classes\n",
    "\n",
    "    arcpy.Delete_management('trfpt_lyr')\n",
    "    # Output to fc_akuta all trees in fc_in with AVSTAND_FAS <= 1\n",
    "    fc_in_path = os.path.join(gdb, fc_in)\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(\n",
    "        fc_in_path, gdb, fc_akuta, '\"AVSTAND_FAS\" <= 1', '#', '#')\n",
    "\n",
    "    # Output to fc_trfpt all trees in fc_in with AVSTAND_FAS > 1\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(\n",
    "        fc_in_path, gdb, fc_trfpt, '\"AVSTAND_FAS\" > 1', '#', '#')\n",
    "    # Then select and delete trees from fc_trfpt that intersect intrangsersatt mark\n",
    "    fc_trfpt_path = os.path.join(gdb, fc_trfpt)\n",
    "    arcpy.MakeFeatureLayer_management(fc_trfpt_path, 'trfpt_lyr')\n",
    "    arcpy.SelectLayerByLocation_management(\n",
    "        'trfpt_lyr', 'INTERSECT', fc_intr_ers, '#', 'NEW_SELECTION')\n",
    "    arcpy.DeleteFeatures_management('trfpt_lyr')\n",
    "    arcpy.Delete_management('trfpt_lyr')\n",
    "\n",
    "\n",
    "def pick_out_akuta(gdb, fc_in, fc_akuta):\n",
    "    # Function for exporting akuta trad (avst_fas <= 1)\n",
    "    fc_in_path = os.path.join(gdb, fc_in)\n",
    "    # Export trees with avst_fas <= 1 to fc_akuta\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(\n",
    "        fc_in_path, gdb, fc_akuta, '\"AVSTAND_FAS\" <= 1', '#', '#')\n",
    "\n",
    "\n",
    "def pick_out_traffpunkter(gdb, fc_in, fc_trfpt, fc_intr_ers):\n",
    "    # function for selection traffpunkter and save in separate feature class\n",
    "    arcpy.Delete_management('trfpt_lyr')\n",
    "\n",
    "    fc_in_path = os.path.join(gdb, fc_in)\n",
    "\n",
    "    # Output to fc_trfpt all trees in fc_in with AVSTAND_FAS > 1\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(\n",
    "        fc_in_path, gdb, fc_trfpt, '\"AVSTAND_FAS\" > 1', '#', '#')\n",
    "    # Then select and delete trees from fc_trfpt that intersect intrangsersatt mark\n",
    "    fc_trfpt_path = os.path.join(gdb, fc_trfpt)\n",
    "    arcpy.MakeFeatureLayer_management(fc_trfpt_path, 'trfpt_lyr')\n",
    "    arcpy.SelectLayerByLocation_management(\n",
    "        'trfpt_lyr', 'INTERSECT', fc_intr_ers, '#', 'NEW_SELECTION')\n",
    "    arcpy.DeleteFeatures_management('trfpt_lyr')\n",
    "    arcpy.Delete_management('trfpt_lyr')\n",
    "\n",
    "\n",
    "def pick_out_ej_akuta(gdb, fc_in, fc_ej_akuta):\n",
    "    # function for selection traffpunkter and save in separate feature class\n",
    "    fc_in_path = os.path.join(gdb, fc_in)\n",
    "    # Export trees with avst_fas > 1 to fc_ej_akuta\n",
    "    arcpy.FeatureClassToFeatureClass_conversion(\n",
    "        fc_in_path, gdb, fc_ej_akuta, '\"AVSTAND_FAS\" > 1', '#', '#')\n",
    "\n",
    "\n",
    "#def populate_SKB_XYZ(input_fc, output_fc, LG, atgardsar, leverantor, ins_met, matosak_p, matosak_h):\n",
    "def populate_SKB_XYZ(input_fc, output_fc, LG, ins_met, matosak_p, matosak_h):\n",
    "    # reads the LG's fc_XYZmZ and writes to the LG'x fc_XYZ\"\n",
    "\n",
    "    LG_code_dict = {1: 10, 2: 20, 3: 30, 4: 40, 5: 50, 6: 60, 7: 70, 8: 80, 9: 90,\n",
    "                    10: 100, 11: 110, 12: 120, 13: 130, 14: 140, 15: 150, 16: 160, 17: 170, 18: 180, 19: 190,\n",
    "                    20: 200, 21: 210, 22: 220, 23: 230, 24: 240, 25: 250, 26: 260, 27: 270, 28: 280, 29: 290,\n",
    "                    30: 300, 31: 310, 32: 320, 33: 330, 34: 340, 35: 350, 36: 360, 37: 370, 38: 380, 39: 390,\n",
    "                    40: 400, 41: 410, 42: 420, 43: 430, 44: 440, 45: 450, 46: 460, 47: 470, 48: 480, 49: 490,\n",
    "                    50: 500, 51: 510, 52: 520, 53: 530, 54: 540, 55: 550, 56: 560, 57: 570, 58: 580, 59: 590,\n",
    "                    60: 600, 61: 610, 62: 620, 63: 630, 64: 640, 65: 650, 66: 660, 67: 670, 68: 680, 69: 690,\n",
    "                    70: 700, 71: 710, 72: 720, 73: 730, 74: 740, 75: 750, 76: 760, 77: 770, 78: 780, 79: 790,\n",
    "                    80: 800, 81: 810, 82: 820, 83: 830, 84: 840, 85: 850, 86: 860, 87: 870, 88: 880, 89: 890,\n",
    "                    90: 900, 91: 910, 92: 920, 93: 930, 94: 940, 95: 950, 96: 951, 97: 952, 98: 953,\n",
    "                    100: 960, 101: 970, 102: 980, 103: 990, 104: 1010, 105: 1020, 106: 1030, 107: 1040, 108: 1050, 109: 1060,\n",
    "                    110: 1070, 111: 1080, 112: 1090, 113: 1100, 114: 1110, 115: 1120, 116: 1130, 117: 1140, 118: 1150, 119: 1160,\n",
    "                    120: 1170, 121: 1180, 122: 1190, 123: 1200, 124: 1210, 125: 1220, 126: 1230, 127: 1240, 128: 1250, 129: 1260,\n",
    "                    130: 1270, 131: 1280, 132: 1290, 133: 1300, 134: 1310, 135: 1320, 136: 1330, 137: 1340, 138: 1350, 139: 1360,\n",
    "                    140: 1370, 141: 1380, 142: 1390, 143: 1400,\n",
    "                    410: 1410, 500: 1420, 550: 1430, 555: 1435, 557: 1440, 661: 1450,\n",
    "                    700: 1460, 701: 1470, 710: 1480, 720: 1490, 720: 1500, 730: 1510, 740: 1520, 750: 1530, 770: 1540}\n",
    "\n",
    "    # in_fields = ['SHAPE@XY', 'Z', 'dZ', 'AVSTAND_HORISONTELLT', 'AVST_MZ_FAS']\n",
    "    in_fields = ['SHAPE@XY', 'SHAPE@Z', 'Z', 'dZ', 'AVSTAND_HORISONTELLT', 'AVST_MZ_FAS']\n",
    "\n",
    "   # out_fields = ['SHAPE@XY', 'SHAPE@Z', 'DELTA_HOJD', 'AVSTAND_FAS', 'MAX_TILLVAXT',\n",
    "   #               'AVSTAND_HORISONTELLT', 'LEDNINGSGATA', 'ATGARDSAR', 'LEVERANTOR',\n",
    "   #               'INSAMLINGSMETOD', 'MATOSAKERHET_PLAN', 'MATOSAKERHET_HOJD']\n",
    "    out_fields = ['SHAPE@XY', 'SHAPE@Z', 'DELTA_HOJD', 'AVSTAND_FAS', \n",
    "                  'AVSTAND_HORISONTELLT', 'LEDNINGSGATA', 'INSAMLINGSMETOD', \n",
    "                  'MATOSAKERHET_PLAN', 'MATOSAKERHET_HOJD']\n",
    "\n",
    "    LG_code = LG_code_dict[LG]\n",
    "\n",
    "    with arcpy.da.SearchCursor(input_fc, in_fields) as s_cursor:\n",
    "        with arcpy.da.InsertCursor(output_fc, out_fields) as i_cursor:\n",
    "            for row in s_cursor:\n",
    "                xy = row[0]\n",
    "                z = row[1]\n",
    "                delta_hojd = row[3]\n",
    "                avst_fas = (row[5] - row[3])\n",
    "                avst_hori = row[4]\n",
    "                \n",
    "               # avst_fas = (row[4] - row[2])\n",
    "               # max_tillv = avst_fas - 1\n",
    "               # delta_hojd = row[2]\n",
    "               # avst_hori = row[3]\n",
    "               # z = row[1]  # - row[2]\n",
    "               # out_row = [row[0], z, delta_hojd, avst_fas, max_tillv, avst_hori, LG_code, atgardsar,\n",
    "               #            leverantor, ins_met, matosak_p, matosak_h]\n",
    "                \n",
    "                out_row = [xy, z, delta_hojd, avst_fas, avst_hori, LG_code, ins_met, matosak_p, matosak_h]\n",
    "\n",
    "                i_cursor.insertRow(out_row)\n",
    "\n",
    "\n",
    "def remove_duplicates(gdb, fc_in, fc_out, xy_tol, z_tol):\n",
    "    # Removes duplicates among akuta and traffpunkter, based on location.\n",
    "    # Assumes that all instances in a \"duplicate group\" have identical\n",
    "    # AVST_FAS etc (true if mZ was calculated for whole LG (all batches)\n",
    "    # together, on all wires in the LG)\n",
    "    input_fc = os.path.join(gdb, fc_in)\n",
    "    no_dupl_fc = os.path.join(gdb, fc_out)\n",
    "\n",
    "    arcpy.Copy_management(input_fc, no_dupl_fc)\n",
    "\n",
    "    arcpy.DeleteIdentical_management(no_dupl_fc, ['Shape'], xy_tol, z_tol)\n",
    "\n",
    "\n",
    "def compute_horizontal_dist(fc_in, wires, radius):\n",
    "    # NEAR_DIST computed by Near3D, and then used as AvstHori in populate_SKB_XYZ,\n",
    "    # does not seem to be the perpendicular distance to the wire, but rather the\n",
    "    # distance to the nearest point on the wire (due to sag, I guess)\n",
    "    # So: Use NEAR analysis in 2D to create a new NEAR column, then use that one as AvstHori\n",
    "    arcpy.Near_analysis(in_features=fc_in, near_features=wires, search_radius=radius,\n",
    "                        location=\"NO_LOCATION\", angle=\"NO_ANGLE\", method=\"PLANAR\")\n",
    "\n",
    "\n",
    "def update_z_coordinate(fd, fc):\n",
    "    \"\"\"New in 2023\n",
    "\n",
    "    Updating z-coordinate from beeing treetop height to\n",
    "    ground point z\n",
    "    \"\"\"\n",
    "\n",
    "    arcpy.env.workspace = os.path.join(fd, fc)\n",
    "    field = ['SHAPE@Z']\n",
    "    cursor = arcpy.UpdateCursor(fc, field)\n",
    "\n",
    "    s_field = ['DELTA_HOJD']\n",
    "    s_cursor = arcpy.SearchCursor(fc, s_field)\n",
    "\n",
    "    with arcpy.da.SearchCursor(fc, s_field) as s_cursor:\n",
    "        with arcpy.da.UpdateCursor(fc, field) as cursor:\n",
    "            for s_row, row in zip(s_cursor, cursor):\n",
    "                row[0] = (row[0] - s_row[0])  # float(s_row[0])\n",
    "                cursor.updateRow(row)\n",
    "\n",
    "        # arcpy.AddField_management(fc,\n",
    "        #                          \"z\", field_type=\"FLOAT\")\n",
    "        # geo = ['!Shape!.firstpoint.z']\n",
    "        # arcpy.CalculateField_management(fc, \"z\", geo[0])\n",
    "\n",
    "\n",
    "def update_horizontal_dist(fc_in):\n",
    "    # put the horizontal distance calculated in compute_horizontal_dist\n",
    "    # in the AvstHori field, and then remove the NEAR field\n",
    "    near_field = 'NEAR_DIST'\n",
    "    near_fid_field = 'NEAR_FID'\n",
    "    avst_hori_field = 'AVSTAND_HORISONTELLT'\n",
    "    update_fields = [avst_hori_field, near_field]\n",
    "\n",
    "    with arcpy.da.UpdateCursor(fc_in, update_fields) as u_cursor:\n",
    "        for row in u_cursor:\n",
    "            row[0] = row[1]\n",
    "            u_cursor.updateRow(row)\n",
    "\n",
    "    arcpy.DeleteField_management(in_table=fc_in, drop_field=near_field)\n",
    "    arcpy.DeleteField_management(in_table=fc_in, drop_field=near_fid_field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import fileinput\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def create_dir(*dirpaths):\n",
    "    import os\n",
    "    for dirpath in dirpaths:\n",
    "        if not os.path.exists(dirpath):\n",
    "            os.makedirs(dirpath)\n",
    "            print(f\"{dirpath}  created.\")\n",
    "        else:\n",
    "            print(f\"{dirpath}  already exists. Not overwritten.\")\n",
    "\n",
    "\n",
    "def copy_file(src, dest):\n",
    "    try:\n",
    "        shutil.copy(src, dest)\n",
    "    # e.g. src and dest are the same file\n",
    "    except shutil.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    # e.g. src or dest doesn't exist\n",
    "    except IOError as e:\n",
    "        print(f\"Error: e.strerror\")\n",
    "\n",
    "\n",
    "def merge_shapefiles(infiles):\n",
    "    import geopandas as gpd\n",
    "    import pandas as pd\n",
    "    indata = [gpd.read_file(infile) for infile in infiles]\n",
    "    indata_combined = gpd.GeoDataFrame(pd.concat(indata, ignore_index=True), crs=indata[0].crs)\n",
    "    return(indata_combined)\n",
    "\n",
    "\n",
    "def merge_blocks(src_dir, search_pattern, dst_file):\n",
    "    blocks = glob.glob(os.path.join(src_dir, search_pattern))\n",
    "    with open(dst_file, \"w\") as fh:\n",
    "        input_lines = fileinput.input(blocks)\n",
    "        fh.writelines(input_lines)\n",
    "        \n",
    "\n",
    "def replace_line_in_file(infile, newfile, oldLine, newLine):\n",
    "    with open(infile, 'r') as input_file, open(newfile, 'w') as output_file:\n",
    "        for line in input_file:\n",
    "            if line.strip() == oldLine:\n",
    "                output_file.write(f\"{newLine}\\n\")\n",
    "            else:\n",
    "                output_file.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Projektsökvägar\n",
    "Sätter nödvändiga sökvägar som kommer att användas genom hela skriptets gång."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### KARIN LOKALT ###\n",
    "#import os\n",
    "#from pathlib import Path\n",
    "#prj_dir = Path(r\"C:\\SVK_2024\\pythonkörningar\")\n",
    "#\n",
    "## Namn på styrfil med information om de ledningar som ska köras:\n",
    "#LGs_info_name = 'veg_kanttrad_241217.txt'\n",
    "#LGs_info_path = os.path.join(r\"Q:\\Projekt\\Data_2024\\styrfiler\", LGs_info_name)\n",
    "#\n",
    "## Namn på den gdb där data ska läggas. Den måste finnas och ha domäner inlagda. \n",
    "## Mall att kopiera finns i GitHub-repot under python\\gdb-mallar-och-domänkoder.\n",
    "#gdb_name = 'veg_kantträd_241217.gdb'\n",
    "#gdb = os.path.join(prj_dir, gdb_name)\n",
    "#\n",
    "## uppdragsmappens mapp \"ledningar\" där alla terrascan-resultat hamnar:\n",
    "#serv_prj_dir = Path(r\"Q:\\Projekt\\Analys_2024\\ledningar\")\n",
    "#\n",
    "## domäner som används:\n",
    "#cvd_LEDNINGSGATA = 'cvd_LEDNINGSGATA.txt'\n",
    "#cvd_LEDNINGSGATA_path = os.path.join(r\"C:\\Users\\SEKNNO\\OneDrive - Sweco AB\\SVK_2024\\30047770-SVK-Analys-av-laserdata-extern\\python\\gdb-mallar-och-domänkoder\", \n",
    "#                                     cvd_LEDNINGSGATA)\n",
    "#skanningsdatum_path = r\"Q:\\Projekt\\Data_2024\\skanningsdatum\\skanningsdatum_241217_temp.txt\"\n",
    "#\n",
    "#\n",
    "##all_merged_name = \"2024_11_11\"\n",
    "##version = \"2024_11_11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Merge SKB text files for all blocks of a powerline\n",
    "Kör blockskript i respektive servermapp för att ha den absolut senaste uppdaterade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing LG022_1\n",
      "Doing LG022_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import fileinput\n",
    "from pathlib import Path\n",
    "#sys.path.insert(1, r\"C:\\Users\\semnve\\Documents\\Visual Studio Code\\30047770-SVK-Analys-av-laserdata\")\n",
    "#import SKB\n",
    "#import SVK\n",
    "import pandas as pd\n",
    "\n",
    "def combine_blocks(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    \n",
    "    print(f\"Doing {LG}_{line}\")\n",
    "    block_dir = os.path.join(line_dir, \"kantträd\", \"block\")\n",
    "    combined_blocks_path = os.path.join(line_dir, \"kantträd\", \"SKB_raw.txt\")\n",
    "    \n",
    "    # Merge files for all blocks into one\n",
    "    #SVK.merge_blocks(block_dir, \"*.txt\", combined_blocks_path)\n",
    "    merge_blocks(block_dir, \"*.txt\", combined_blocks_path)\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "powerlines_df.apply(combine_blocks, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Kopiera skb_raw.txt\n",
    "Denna kopierar över alla skb_raw.txt till den temporära mappen på skrivbordet"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
=======
   "execution_count": 4,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "True\n",
      "True\n"
=======
      "Successfully inserted edge trees to PostGIS database.\n",
      "Connection to database leverans_250425 closed.\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "import os\n",
    "import filecmp\n",
    "import shutil\n",
=======
    "import psycopg2\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from psycopg2 import sql\n",
    "\n",
    "cur = None\n",
    "conn = None\n",
    "\n",
    "def create_SKB_XYmZ_table(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    SKB_dir = line_dir / \"kantträd\"\n",
    "    SKB_file_in = os.path.join(SKB_dir, f\"SKB_raw.txt\")\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name))\n",
    "    create_query = sql.SQL(\"CREATE TABLE {table_name}(objectid SERIAL PRIMARY KEY, x DOUBLE PRECISION, y DOUBLE PRECISION, z DOUBLE PRECISION, delta_hojd REAL, mz DOUBLE PRECISION, shape geometry(POINTZ, 3006))\").format(table_name=sql.Identifier(table_name)) #TODO potentially change to 5845\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create table for powerline\n",
    "        execute_query(conn_db, drop_table_if_exists_query)\n",
    "        execute_query(conn_db, create_query)\n",
    "        \n",
    "        # Step 2: Insert edge trees into powerline table \n",
    "        with open(SKB_file_in, 'r') as src_file:\n",
    "            for file_line in src_file:\n",
    "                l_split = file_line.split(' ')\n",
    "                x = float(l_split[0])\n",
    "                y = float(l_split[1])\n",
    "                z = float(l_split[2])\n",
    "                dz = float(l_split[3])\n",
    "                mz = z - dz\n",
    "                \n",
    "                #TODO potentially change to 5845, also check if SRID is needed on table level as well\n",
    "                # TODO check if I should change to ST_SetSRID(ST_MakePoint(x, y, z), 3006)\n",
    "                insert_query = sql.SQL(\"\"\"\n",
    "                    INSERT INTO {table_name} (x, y, z, delta_hojd, mz, shape)\n",
    "                    VALUES (%s, %s, %s, %s, %s, ST_GeomFromText(%s, 3006))\n",
    "                \"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "                pointz_string = f\"POINTZ({x} {y} {mz})\"\n",
    "                tree_data = (x, y, z, dz, mz, pointz_string)\n",
    "                execute_query(conn_db, insert_query, tree_data)\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(create_SKB_XYmZ_table, axis=1)\n",
    "    \n",
    "    print(\"Successfully inserted edge trees to PostGIS database.\") \n",
    "\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns were added successfully to table lg037_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG037_1\n",
      "avstand_fas was calculated succesfully for LG037_1\n",
      "avstand_horisontellt was calculated succesfully for LG037_1\n",
      "Columns were added successfully to table lg037_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG037_2\n",
      "avstand_fas was calculated succesfully for LG037_2\n",
      "avstand_horisontellt was calculated succesfully for LG037_2\n",
      "Columns were added successfully to table lg037_3_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG037_3\n",
      "avstand_fas was calculated succesfully for LG037_3\n",
      "avstand_horisontellt was calculated succesfully for LG037_3\n",
      "Columns were added successfully to table lg037_4_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG037_4\n",
      "avstand_fas was calculated succesfully for LG037_4\n",
      "avstand_horisontellt was calculated succesfully for LG037_4\n",
      "Columns were added successfully to table lg037_5_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG037_5\n",
      "avstand_fas was calculated succesfully for LG037_5\n",
      "avstand_horisontellt was calculated succesfully for LG037_5\n",
      "Columns were added successfully to table lg037_6_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG037_6\n",
      "avstand_fas was calculated succesfully for LG037_6\n",
      "avstand_horisontellt was calculated succesfully for LG037_6\n",
      "Columns were added successfully to table lg038_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG038_1\n",
      "avstand_fas was calculated succesfully for LG038_1\n",
      "avstand_horisontellt was calculated succesfully for LG038_1\n",
      "Columns were added successfully to table lg038_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG038_2\n",
      "avstand_fas was calculated succesfully for LG038_2\n",
      "avstand_horisontellt was calculated succesfully for LG038_2\n",
      "Columns were added successfully to table lg038_3_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG038_3\n",
      "avstand_fas was calculated succesfully for LG038_3\n",
      "avstand_horisontellt was calculated succesfully for LG038_3\n",
      "Columns were added successfully to table lg040_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_1\n",
      "avstand_fas was calculated succesfully for LG040_1\n",
      "avstand_horisontellt was calculated succesfully for LG040_1\n",
      "Columns were added successfully to table lg040_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_2\n",
      "avstand_fas was calculated succesfully for LG040_2\n",
      "avstand_horisontellt was calculated succesfully for LG040_2\n",
      "Columns were added successfully to table lg040_3_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_3\n",
      "avstand_fas was calculated succesfully for LG040_3\n",
      "avstand_horisontellt was calculated succesfully for LG040_3\n",
      "Columns were added successfully to table lg040_4_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_4\n",
      "avstand_fas was calculated succesfully for LG040_4\n",
      "avstand_horisontellt was calculated succesfully for LG040_4\n",
      "Columns were added successfully to table lg040_5_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_5\n",
      "avstand_fas was calculated succesfully for LG040_5\n",
      "avstand_horisontellt was calculated succesfully for LG040_5\n",
      "Columns were added successfully to table lg040_6_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_6\n",
      "avstand_fas was calculated succesfully for LG040_6\n",
      "avstand_horisontellt was calculated succesfully for LG040_6\n",
      "Columns were added successfully to table lg040_7_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_7\n",
      "avstand_fas was calculated succesfully for LG040_7\n",
      "avstand_horisontellt was calculated succesfully for LG040_7\n",
      "Columns were added successfully to table lg040_8_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG040_8\n",
      "avstand_fas was calculated succesfully for LG040_8\n",
      "avstand_horisontellt was calculated succesfully for LG040_8\n",
      "Columns were added successfully to table lg041_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG041_1\n",
      "avstand_fas was calculated succesfully for LG041_1\n",
      "avstand_horisontellt was calculated succesfully for LG041_1\n",
      "Columns were added successfully to table lg041_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG041_2\n",
      "avstand_fas was calculated succesfully for LG041_2\n",
      "avstand_horisontellt was calculated succesfully for LG041_2\n",
      "Columns were added successfully to table lg041_3_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG041_3\n",
      "avstand_fas was calculated succesfully for LG041_3\n",
      "avstand_horisontellt was calculated succesfully for LG041_3\n",
      "Columns were added successfully to table lg041_4_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG041_4\n",
      "avstand_fas was calculated succesfully for LG041_4\n",
      "avstand_horisontellt was calculated succesfully for LG041_4\n",
      "Columns were added successfully to table lg041_5_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG041_5\n",
      "avstand_fas was calculated succesfully for LG041_5\n",
      "avstand_horisontellt was calculated succesfully for LG041_5\n",
      "Columns were added successfully to table lg042_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG042_1\n",
      "avstand_fas was calculated succesfully for LG042_1\n",
      "avstand_horisontellt was calculated succesfully for LG042_1\n",
      "Columns were added successfully to table lg056_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG056_1\n",
      "avstand_fas was calculated succesfully for LG056_1\n",
      "avstand_horisontellt was calculated succesfully for LG056_1\n",
      "Columns were added successfully to table lg105_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG105_1\n",
      "avstand_fas was calculated succesfully for LG105_1\n",
      "avstand_horisontellt was calculated succesfully for LG105_1\n",
      "Columns were added successfully to table lg105_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG105_2\n",
      "avstand_fas was calculated succesfully for LG105_2\n",
      "avstand_horisontellt was calculated succesfully for LG105_2\n",
      "Columns were added successfully to table lg137_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG137_1\n",
      "avstand_fas was calculated succesfully for LG137_1\n",
      "avstand_horisontellt was calculated succesfully for LG137_1\n",
      "Columns were added successfully to table lg137_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG137_2\n",
      "avstand_fas was calculated succesfully for LG137_2\n",
      "avstand_horisontellt was calculated succesfully for LG137_2\n",
      "Columns were added successfully to table lg139_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG139_1\n",
      "avstand_fas was calculated succesfully for LG139_1\n",
      "avstand_horisontellt was calculated succesfully for LG139_1\n",
      "Columns were added successfully to table lg139_2_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG139_2\n",
      "avstand_fas was calculated succesfully for LG139_2\n",
      "avstand_horisontellt was calculated succesfully for LG139_2\n",
      "Columns were added successfully to table lg140_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG140_1\n",
      "avstand_fas was calculated succesfully for LG140_1\n",
      "avstand_horisontellt was calculated succesfully for LG140_1\n",
      "Successfully calculated distances for all wires.\n",
      "Connection to database leverans_250425 closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pathlib import Path\n",
>>>>>>> Stashed changes
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "to_dir = os.path.join(prj_dir, \"ledningar\")\n",
    "from_dir = serv_prj_dir\n",
    "\n",
    "LGs_info = pd.read_csv(LGs_info_path, sep=\"\\t\", header=0)\n",
    "from_dir_SKB = []\n",
    "for i, j in zip(LGs_info[\"LG\"], LGs_info[\"line\"]):\n",
    "    from_dir_SKB.append(f'{i}\\\\line_{j}\\\\Kantträd')\n",
    "to_dir_SKB = []\n",
    "for i, j in zip(LGs_info[\"LG\"], LGs_info[\"line\"]):\n",
    "    to_dir_SKB.append(f'{i}\\\\line_{j}\\\\SKB')\n",
    "\n",
    "outlist_to_dir = []\n",
    "for i in to_dir_SKB:\n",
    "    outlist_to_dir.append((f\"{to_dir}\\{i}\\SKB_raw.txt\").replace('\\\\\\\\', '\\\\'))\n",
    "\n",
    "outlist_from_dir = []\n",
    "for i in from_dir_SKB:\n",
    "    outlist_from_dir.append(\n",
    "        (f\"{from_dir}\\{i}\\SKB_raw.txt\").replace('\\\\\\\\', '\\\\'))\n",
    "\n",
<<<<<<< Updated upstream
    "for i, j in zip(outlist_from_dir, outlist_to_dir):\n",
    "    if os.path.exists(j.replace('\\\\\\\\', '\\\\')):\n",
    "        shutil.copyfile(i.replace('\\\\\\\\', '\\\\'), j.replace('\\\\\\\\', '\\\\'))\n",
    "    else:\n",
    "        folder1 = j.split(\"\\\\\")[-4]\n",
    "        folder2 = j.split(\"\\\\\")[-3]\n",
    "        folder3 = j.split(\"\\\\\")[-2]\n",
    "        os.makedirs(\n",
    "            os.path.join(\n",
    "                to_dir,\n",
    "                folder1,\n",
    "                folder2,\n",
    "                folder3,\n",
    "                #\"SKB\"\n",
=======
    "    # Step 1: Add attributes\n",
    "    execute_query(conn_db, add_columns_query)\n",
    "    print(f\"Columns were added successfully to table {table_name}\")\n",
    "\n",
    "    # Step 2: Calculate shortest distance between mz and phase.\n",
    "    execute_query(conn_db, avst_mz_fas_query)\n",
    "    print(f\"avst_mz_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 3: Calculate the shortest distance between tree top (z) and phase with event of potential fall.\n",
    "    execute_query(conn_db, avst_fas_query)\n",
    "    print(f\"avstand_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 4: Calculate the horizontal distance between tree and phase.\n",
    "    execute_query(conn_db, avst_hori_query)\n",
    "    print(f\"avstand_horisontellt was calculated succesfully for {LG}_{line}\") \n",
    "\n",
    "try: \n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(lambda row: calculate_distances(row, conn_db), axis=1)\n",
    "    \n",
    "    print(\"Successfully calculated distances for all wires.\") #TODO printed on error, change this\n",
    "except psycopg.Error as e:\n",
    "    print(\"Error while working with PostgreSQL:\", e) \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Littera, Ursprung, Ursprung_Datum, Registreringsdatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns were added successfully to table lg037_1_skb_xymz\n",
      "Values were updated successfully in table lg037_1_skb_xymz\n",
      "Columns were added successfully to table lg037_2_skb_xymz\n",
      "Values were updated successfully in table lg037_2_skb_xymz\n",
      "Columns were added successfully to table lg037_3_skb_xymz\n",
      "Values were updated successfully in table lg037_3_skb_xymz\n",
      "Columns were added successfully to table lg037_4_skb_xymz\n",
      "Values were updated successfully in table lg037_4_skb_xymz\n",
      "Columns were added successfully to table lg037_5_skb_xymz\n",
      "Values were updated successfully in table lg037_5_skb_xymz\n",
      "Columns were added successfully to table lg037_6_skb_xymz\n",
      "Values were updated successfully in table lg037_6_skb_xymz\n",
      "Columns were added successfully to table lg038_1_skb_xymz\n",
      "Values were updated successfully in table lg038_1_skb_xymz\n",
      "Columns were added successfully to table lg038_2_skb_xymz\n",
      "Values were updated successfully in table lg038_2_skb_xymz\n",
      "Columns were added successfully to table lg038_3_skb_xymz\n",
      "Values were updated successfully in table lg038_3_skb_xymz\n",
      "Columns were added successfully to table lg040_1_skb_xymz\n",
      "Values were updated successfully in table lg040_1_skb_xymz\n",
      "Columns were added successfully to table lg040_2_skb_xymz\n",
      "Values were updated successfully in table lg040_2_skb_xymz\n",
      "Columns were added successfully to table lg040_3_skb_xymz\n",
      "Values were updated successfully in table lg040_3_skb_xymz\n",
      "Columns were added successfully to table lg040_4_skb_xymz\n",
      "Values were updated successfully in table lg040_4_skb_xymz\n",
      "Columns were added successfully to table lg040_5_skb_xymz\n",
      "Values were updated successfully in table lg040_5_skb_xymz\n",
      "Columns were added successfully to table lg040_6_skb_xymz\n",
      "Values were updated successfully in table lg040_6_skb_xymz\n",
      "Columns were added successfully to table lg040_7_skb_xymz\n",
      "Values were updated successfully in table lg040_7_skb_xymz\n",
      "Columns were added successfully to table lg040_8_skb_xymz\n",
      "Values were updated successfully in table lg040_8_skb_xymz\n",
      "Columns were added successfully to table lg041_1_skb_xymz\n",
      "Values were updated successfully in table lg041_1_skb_xymz\n",
      "Columns were added successfully to table lg041_2_skb_xymz\n",
      "Values were updated successfully in table lg041_2_skb_xymz\n",
      "Columns were added successfully to table lg041_3_skb_xymz\n",
      "Values were updated successfully in table lg041_3_skb_xymz\n",
      "Columns were added successfully to table lg041_4_skb_xymz\n",
      "Values were updated successfully in table lg041_4_skb_xymz\n",
      "Columns were added successfully to table lg041_5_skb_xymz\n",
      "Values were updated successfully in table lg041_5_skb_xymz\n",
      "Columns were added successfully to table lg042_1_skb_xymz\n",
      "Values were updated successfully in table lg042_1_skb_xymz\n",
      "Columns were added successfully to table lg056_1_skb_xymz\n",
      "Values were updated successfully in table lg056_1_skb_xymz\n",
      "Columns were added successfully to table lg105_1_skb_xymz\n",
      "Values were updated successfully in table lg105_1_skb_xymz\n",
      "Columns were added successfully to table lg105_2_skb_xymz\n",
      "Values were updated successfully in table lg105_2_skb_xymz\n",
      "Columns were added successfully to table lg137_1_skb_xymz\n",
      "Values were updated successfully in table lg137_1_skb_xymz\n",
      "Columns were added successfully to table lg137_2_skb_xymz\n",
      "Values were updated successfully in table lg137_2_skb_xymz\n",
      "Columns were added successfully to table lg139_1_skb_xymz\n",
      "Values were updated successfully in table lg139_1_skb_xymz\n",
      "Columns were added successfully to table lg139_2_skb_xymz\n",
      "Values were updated successfully in table lg139_2_skb_xymz\n",
      "Columns were added successfully to table lg140_1_skb_xymz\n",
      "Values were updated successfully in table lg140_1_skb_xymz\n",
      "Successfully added columns and updated values.\n",
      "Connection to database leverans_250425 closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2 as psycopg\n",
    "import pandas as pd\n",
    "from psycopg2 import sql\n",
    "from datetime import date\n",
    "\n",
    "INSAMLINGSSMETOD = 20\n",
    "MATOSAKERHET_P = 1000\n",
    "MATOSAKERHET_H = 1000\n",
    "LG_CODE_DICT = {1: 10, 2: 20, 3: 30, 4: 40, 5: 50, 6: 60, 7: 70, 8: 80, 9: 90,\n",
    "                10: 100, 11: 110, 12: 120, 13: 130, 14: 140, 15: 150, 16: 160, 17: 170, 18: 180, 19: 190,\n",
    "                20: 200, 21: 210, 22: 220, 23: 230, 24: 240, 25: 250, 26: 260, 27: 270, 28: 280, 29: 290,\n",
    "                30: 300, 31: 310, 32: 320, 33: 330, 34: 340, 35: 350, 36: 360, 37: 370, 38: 380, 39: 390,\n",
    "                40: 400, 41: 410, 42: 420, 43: 430, 44: 440, 45: 450, 46: 460, 47: 470, 48: 480, 49: 490,\n",
    "                50: 500, 51: 510, 52: 520, 53: 530, 54: 540, 55: 550, 56: 560, 57: 570, 58: 580, 59: 590,\n",
    "                60: 600, 61: 610, 62: 620, 63: 630, 64: 640, 65: 650, 66: 660, 67: 670, 68: 680, 69: 690,\n",
    "                70: 700, 71: 710, 72: 720, 73: 730, 74: 740, 75: 750, 76: 760, 77: 770, 78: 780, 79: 790,\n",
    "                80: 800, 81: 810, 82: 820, 83: 830, 84: 840, 85: 850, 86: 860, 87: 870, 88: 880, 89: 890,\n",
    "                90: 900, 91: 910, 92: 920, 93: 930, 94: 940, 95: 950, 96: 951, 97: 952, 98: 953,\n",
    "                100: 960, 101: 970, 102: 980, 103: 990, 104: 1010, 105: 1020, 106: 1030, 107: 1040, 108: 1050, 109: 1060,\n",
    "                110: 1070, 111: 1080, 112: 1090, 113: 1100, 114: 1110, 115: 1120, 116: 1130, 117: 1140, 118: 1150, 119: 1160,\n",
    "                120: 1170, 121: 1180, 122: 1190, 123: 1200, 124: 1210, 125: 1220, 126: 1230, 127: 1240, 128: 1250, 129: 1260,\n",
    "                130: 1270, 131: 1280, 132: 1290, 133: 1300, 134: 1310, 135: 1320, 136: 1330, 137: 1340, 138: 1350, 139: 1360,\n",
    "                140: 1370, 141: 1380, 142: 1390, 143: 1400,\n",
    "                410: 1410, 500: 1420, 550: 1430, 555: 1435, 557: 1440, 661: 1450,\n",
    "                700: 1460, 701: 1470, 710: 1480, 720: 1490, 720: 1500, 730: 1510, 740: 1520, 750: 1530, 770: 1540}\n",
    "\n",
    "def add_data(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    littera = row[\"Littera\"]\n",
    "    LG_nr = int(LG[-3:])\n",
    "    regdatum = skanningsdatum[f\"{LG}_{line}\"]   \n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    \n",
    "    add_columns_query = sql.SQL(\"\"\"ALTER TABLE {table_name}\n",
    "                                    ADD COLUMN IF NOT EXISTS rgdtm DATE,  \n",
    "                                    ADD COLUMN IF NOT EXISTS ledningsgata DOUBLE PRECISION,\n",
    "                                    ADD COLUMN IF NOT EXISTS insamlingsmetod DOUBLE PRECISION,\n",
    "                                    ADD COLUMN IF NOT EXISTS matosakerhet_plan DOUBLE PRECISION,\n",
    "                                    ADD COLUMN IF NOT EXISTS matosakerhet_hojd DOUBLE PRECISION,\n",
    "                                    ADD COLUMN IF NOT EXISTS littera VARCHAR(255),\n",
    "                                    ADD COLUMN IF NOT EXISTS ursprung VARCHAR(255),\n",
    "                                    ADD COLUMN IF NOT EXISTS ursprung_datum DATE;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    # TODO Investigate if % or placeholder should be used\n",
    "    update_query = sql.SQL(\"\"\"UPDATE {table_name}\n",
    "                                SET rgdtm = {regdatum}, \n",
    "                                ledningsgata = {ledningsgata},\n",
    "                                littera = {littera},\n",
    "                                ursprung = {ursprung},\n",
    "                                insamlingsmetod = {insamlingsmetod},\n",
    "                                matosakerhet_plan = {matosakerhet_plan},\n",
    "                                matosakerhet_hojd = {matosakerhet_hojd},\n",
    "                                ursprung_datum = {lev_dat};\"\"\").format(table_name=sql.Identifier(table_name), \n",
    "                                                                       regdatum=sql.Placeholder(\"regdatum\"), \n",
    "                                                                       ledningsgata=sql.Placeholder(\"ledningsgata\"),\n",
    "                                                                       littera=sql.Placeholder(\"littera\"), \n",
    "                                                                       ursprung=sql.Placeholder(\"ursprung\"), \n",
    "                                                                       insamlingsmetod=sql.Placeholder(\"insamlingsmetod\"),\n",
    "                                                                       lev_dat=sql.Placeholder(\"lev_dat\"),\n",
    "                                                                       matosakerhet_plan=sql.Placeholder(\"matosakerhet_plan\"),\n",
    "                                                                       matosakerhet_hojd=sql.Placeholder(\"matosakerhet_hojd\"))\n",
    "    updated_values = {\"regdatum\": regdatum, \"ledningsgata\": LG_CODE_DICT[LG_nr], \"littera\": littera, \"ursprung\": urspr, \"insamlingsmetod\": INSAMLINGSSMETOD, \"lev_dat\": lev_dat, \"matosakerhet_plan\": MATOSAKERHET_P, \"matosakerhet_hojd\": MATOSAKERHET_H}\n",
    "\n",
    "    try:\n",
    "        # Step 1: Add columns\n",
    "        execute_query(conn_db, add_columns_query)\n",
    "        print(f\"Columns were added successfully to table {table_name}\")\n",
    "        \n",
    "        # Step 2: Update Littera, Ursprung, Ursprung_Datum, Registreringsdatum \n",
    "        execute_query(conn_db, update_query, updated_values)\n",
    "        print(f\"Values were updated successfully in table {table_name}\")\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    df_skanningsdatum = pd.read_csv(scandate_file, sep=\"\\t\", header=0) #TODO english variables?\n",
    "    skanningsdatum = {f\"{row['LG']}_{row['line']}\": row['skanningsdatum'] for _, row in df_skanningsdatum.iterrows()}\n",
    "    urspr = \"SWECO\"\n",
    "    lev_dat = str(date.today())\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(add_data, axis=1) #TODO don't have to pass conn_db on the other places, do like this instead\n",
    "    \n",
    "    print(\"Successfully added columns and updated values.\") \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge edge trees for all powerlines into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables: ['lg037_1_skb_xymz', 'lg037_2_skb_xymz', 'lg037_3_skb_xymz', 'lg037_4_skb_xymz', 'lg037_5_skb_xymz', 'lg037_6_skb_xymz', 'lg038_1_skb_xymz', 'lg038_2_skb_xymz', 'lg038_3_skb_xymz', 'lg040_1_skb_xymz', 'lg040_2_skb_xymz', 'lg040_3_skb_xymz', 'lg040_4_skb_xymz', 'lg040_5_skb_xymz', 'lg040_6_skb_xymz', 'lg040_7_skb_xymz', 'lg040_8_skb_xymz', 'lg041_1_skb_xymz', 'lg041_2_skb_xymz', 'lg041_3_skb_xymz', 'lg041_4_skb_xymz', 'lg041_5_skb_xymz', 'lg042_1_skb_xymz', 'lg056_1_skb_xymz', 'lg105_1_skb_xymz', 'lg105_2_skb_xymz', 'lg137_1_skb_xymz', 'lg137_2_skb_xymz', 'lg139_1_skb_xymz', 'lg139_2_skb_xymz', 'lg140_1_skb_xymz']\n",
      "Successfully merged all edge trees tables into one table.\n",
      "Connection to database leverans_250425 closed.\n"
     ]
    }
   ],
   "source": [
    "def list_skb_xymz_tables():\n",
    "    select_tables_query = sql.SQL(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name LIKE '%%skb_xymz' AND table_schema = 'public';\n",
    "    \"\"\")\n",
    "\n",
    "    result = execute_query(conn_db, select_tables_query, fetch=True)\n",
    "    table_names = [t[0] for t in result]\n",
    "\n",
    "    return table_names\n",
    "\n",
    "def merge_tables(table_names):\n",
    "    table_name = 'kantträd_oklippta'\n",
    "\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name)) #TODO maybe these two queries can be combined into one?\n",
    "\n",
    "    create_table_query = sql.SQL(\"\"\"\n",
    "        CREATE TABLE {table_name} AS\n",
    "        TABLE {first_table} WITH NO DATA;\n",
    "    \"\"\").format(table_name=sql.Identifier(table_name), first_table=sql.Identifier(table_names[0]))\n",
    "\n",
    "    union_parts = []\n",
    "    for table in table_names:\n",
    "        part = sql.SQL(\"\"\"\n",
    "            SELECT * FROM (\n",
    "                SELECT *\n",
    "                FROM {table}\n",
    "                ORDER BY objectid\n",
>>>>>>> Stashed changes
    "            )\n",
    "        )\n",
    "        shutil.copyfile(i.replace('\\\\\\\\', '\\\\'), j.replace('\\\\\\\\', '\\\\'))\n",
    "\n",
    "for i, j in zip(outlist_from_dir, outlist_to_dir):\n",
    "    print(filecmp.cmp(i, j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Calculate distances to wire for trees"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
=======
   "execution_count": 11,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Starting with LG022_1\n",
      "LG022_1 faslinor i Wires\n",
      "LG022_1_SKB_XYmZ created\n",
      "done with Near3D LG022_1_SKB_XYmZ\n",
      "done with deletefield LG022_1_SKB_XYmZ\n",
      "dist_mZ_wire calculated for LG022_1\n",
      "13:55:46\n",
      "SKB_XYZ populated\n",
      "Avstand fas computed\n",
      "Avstand horisontellt computed\n",
      "Setting ground Z for XYZ points\n",
      "Done with LG022_1\n",
      "Starting with LG022_2\n",
      "LG022_2 faslinor i Wires\n",
      "LG022_2_SKB_XYmZ created\n",
      "done with Near3D LG022_2_SKB_XYmZ\n",
      "done with deletefield LG022_2_SKB_XYmZ\n",
      "dist_mZ_wire calculated for LG022_2\n",
      "14:14:49\n",
      "SKB_XYZ populated\n",
      "Avstand fas computed\n",
      "Avstand horisontellt computed\n",
      "Setting ground Z for XYZ points\n",
      "Done with LG022_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
=======
      "Successfully converted Q:/Projekt/Data_2024/Underlag_SVK/Bestallningsunderlag_2024.gdb to PostGIS as 'gng_ledningsgata'.\n",
      "Successfully converted Q:/Projekt/Data_2024/Underlag_SVK/Bestallningsunderlag_2024.gdb to PostGIS as 'gng_stationsomrade'.\n",
      "Connection to database leverans_250425 closed.\n"
     ]
>>>>>>> Stashed changes
    }
   ],
   "source": [
    "import sys\n",
    "import os, time, arcpy\n",
    "from time import gmtime, strftime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "#import SKB\n",
    "\n",
    "# Featuredataset för kantträdspunkter innehållande X, Y och markens Z-värde under trädet:\n",
    "SKB_XYmZ_location = os.path.join(working_gdb, 'SKB_XYmZ')\n",
    "\n",
    "# Featuredataset för kantträdspunkter innehållande X, Y och Z-värde för trädkronans topp:\n",
    "SKB_XYZ_location = os.path.join(working_gdb, 'SKB_XYZ')\n",
    "\n",
    "\n",
    "# Featuredataset för faslinor, som i ett tidigare steg lästs in från DGN-filerna (Wires.ipynb):\n",
    "#wires_fd_1 = os.path.join(gdb, 'Wires_2041205')\n",
    "wires_fd_1 = wires_gdb\n",
    "# wires_fd_2 = os.path.join(gdb, 'Wires_incomplete') # Troligen en rest från när vi hade dåliga punktmoln som orsakade luckor i faslinorna\n",
    "wires_suffix = '_fas'\n",
    "search_radius = 100 # How far from the point should Near3D search for wires\n",
    "SKB_template = os.path.join(working_gdb, 'SKB_XYZ', 'SKB_template') # Detta bör vara en mall för featureklassen med resultatet som ska skapas\n",
    "atgardsar = 2024 # TODO: BÖR INTE VARA MED LÄNGRE, KOLLA SÅ ATT DET INTE ANVÄNDS LÄNGRE NER I SKRIPTET\n",
    "leverantor = 'SWECO'\n",
    "insamlingsmetod = 20\n",
    "matosakerhet_p = 1000\n",
    "matosakerhet_h = 1000\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "def sweref99TM():\n",
    "    wkt = 'PROJCS[\"SWEREF99_TM\",GEOGCS[\"GCS_SWEREF99\",DATUM[\"D_SWEREF99\",SPHEROID[\"GRS_1980\",6378137,298.257222101]],PRIMEM[\"Greenwich\",0],UNIT[\"Degree\",0.017453292519943295]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",15],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"Meter\",1]]'\n",
    "    spatialref = arcpy.SpatialReference()\n",
    "    spatialref.loadFromString(wkt)\n",
    "    return(spatialref)\n",
    "\n",
    "\n",
    "def compute_SKB_distances(row):\n",
    "    LG = row[\"LG\"] # TODO: Hänvisa till kolumnnamn i styrfilen istället för index\n",
    "    line = row[\"line\"] # TODO: Hänvisa till kolumnnamn i styrfilen istället för index\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    SKB_dir = line_dir / \"kantträd\" \n",
    "    \n",
    "    arcpy.env.overwriteOutput = True\n",
    "    print(f\"Starting with {LG}_{line}\")\n",
    "    \n",
    "    # Namn på featureklasser som ska skapas i featuredataset som definierats i början av skriptet:\n",
    "    SKB_XYmZ = f\"{LG}_{line}_SKB_XYmZ\"\n",
    "    SKB_XYZ = f\"{LG}_{line}_SKB_XYZ\"\n",
    "    akuta_trad = f\"{LG}_{line}_akuta\"\n",
    "    ej_akuta_trad = f\"{LG}_{line}_ej_akuta\"\n",
    "    \n",
    "    # Sökvägar till featureklasserna:\n",
    "    SKB_XYmZ_path = os.path.join(SKB_XYmZ_location, SKB_XYmZ)\n",
    "    SKB_XYZ_path = os.path.join(SKB_XYZ_location, SKB_XYZ)\n",
    "    akuta_trad_path = os.path.join(SKB_XYZ_location, akuta_trad) # Används ej? Inaktuellt eftersom vi inte längre delar upp kantträd i akuta och ej akuta\n",
    "    ej_akuta_trad_path = os.path.join(SKB_XYZ_location, ej_akuta_trad) # Används ej? Inaktuellt eftersom vi inte längre delar upp kantträd i akuta och ej akuta\n",
    "\n",
    "    wires_1 = os.path.join(wires_fd_1, f\"{LG}_{line}{wires_suffix}\")\n",
    "    # wires_2 = os.path.join(wires_fd_2, f\"{LG}_{line}{wires_suffix}\") # REST SOM INTE LÄNGRE SKA VARA MED? DEN FANNS NÄR VI HADE DÅLIGA PUNKTMOLN SOM ORSAKADE LUCKOR I FASLINORNA\n",
    "    \n",
    "    if arcpy.Exists(wires_1):\n",
    "        wires = wires_1\n",
    "        print(f\"{LG}_{line} faslinor i Wires\")\n",
    "    #elif arcpy.Exists(wires_2):\n",
    "    #    wires = wires_2\n",
    "    #    print(f\"{LG}_{line} faslinor i Wires_incomplete\")\n",
    "    else:\n",
    "        print(f\"Inga faslinor för {LG}_{line}\")\n",
    "        return\n",
    "    \n",
    "    # Create an fc for X,Y,Z,dZ,mZ with mZ as Z coordinate\n",
    "    # Z är trädtoppens Z-värde i RH2000 dvs höjd över havet\n",
    "    # dZ är skillanden mellan markens Z-värde under trädet och trädkronans Z-värde\n",
    "    # mZ är markens Z-värde under trädet\n",
    "    # Output från TerraScan innehåller X, Y, Z och dZ.\n",
    "    SKB_file_in = os.path.join(SKB_dir, f\"SKB_raw.txt\")\n",
    "    #SKB.create_SKB_XYmZ(SKB_file_in, SKB_XYmZ_location, SKB_XYmZ, sr)\n",
    "    create_SKB_XYmZ(SKB_file_in, SKB_XYmZ_location, SKB_XYmZ, sr)\n",
    "    print(f'{LG}_{line}_SKB_XYmZ created')\n",
    "\n",
    "    # Call arcpy function Near3D to calculate distance from mZ to wire, \n",
    "    # on SKB_XYmZ\n",
    "    #SKB.dist_mZ_wire(SKB_XYmZ_location, SKB_XYmZ, wires, search_radius)\n",
    "    dist_mZ_wire(SKB_XYmZ_location, SKB_XYmZ, wires, search_radius)\n",
    "    print(f'dist_mZ_wire calculated for {LG}_{line}')\n",
    "    print(f'{strftime(\"%H:%M:%S\", gmtime())}')\n",
    "\n",
    "    # Create feature class for SKB where point XYZ is the XYZ for the tree tops\n",
    "    #SKB.create_SKB_XYZ(SKB_XYZ_location, SKB_XYZ, SKB_template, sr)\n",
    "    create_SKB_XYZ(SKB_XYZ_location, SKB_XYZ, SKB_template, sr)\n",
    "\n",
    "    # Fill SKB_XYZ with values from SKB_XYmZ and set Z coordinate to Z (instead of mZ)\n",
    "    LG_nr = int(LG[-3:])\n",
    "    #SKB.populate_SKB_XYZ(SKB_XYmZ_path, SKB_XYZ_path, LG_nr, atgardsar, leverantor, insamlingsmetod,\n",
    "    #                    matosakerhet_p, matosakerhet_h)\n",
    "    #populate_SKB_XYZ(SKB_XYmZ_path, SKB_XYZ_path, LG_nr, atgardsar, leverantor, insamlingsmetod, matosakerhet_p, matosakerhet_h)\n",
    "    populate_SKB_XYZ(SKB_XYmZ_path, SKB_XYZ_path, LG_nr, insamlingsmetod, matosakerhet_p, matosakerhet_h)\n",
    "    print('SKB_XYZ populated')  \n",
    "\n",
    "    # NEAR_DIST computed by Near3D, and then used as AvstHori in populate_SKB_XYZ,\n",
    "    # does not seem to be the perpendicular distance to the wire, but rather the \n",
    "    # distance to the nearest point on the wire (due to sag, I guess)\n",
    "    # So: Use NEAR analysis in 2D to create a new NEAR column, then use that one as AvstHori\n",
    "    #SKB.compute_horizontal_dist(SKB_XYZ_path, wires, search_radius)\n",
    "    compute_horizontal_dist(SKB_XYZ_path, wires, search_radius)\n",
    "    print('Avstand fas computed')\n",
    "          \n",
    "    # Now replace old avst_hori with new, correct one from compute_horizontal_dist. \n",
    "    # Then remove the NEAR-fields.\n",
    "    #SKB.update_horizontal_dist(SKB_XYZ_path)\n",
    "    update_horizontal_dist(SKB_XYZ_path)\n",
    "    print('Avstand horisontellt computed')\n",
    "     \n",
    "    print(\"Setting ground Z for XYZ points\")\n",
    "    all_fc = os.path.join(SKB_XYZ_path, SKB_XYZ)\n",
    "    #SKB.update_z_coordinate(SKB_XYZ_location, SKB_XYZ)\n",
    "    update_z_coordinate(SKB_XYZ_location, SKB_XYZ)\n",
    "\n",
    "    \n",
    "          \n",
    "    print(f\"Done with {LG}_{line}\")\n",
    "# Coordinate system\n",
    "sr = sweref99TM()\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "powerlines_df.apply(compute_SKB_distances, axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Städa bland fält och fältnamn\n",
    "## VARFÖR TAS FÄLTEN LEDNINGSGATA OCH INSAMLINGSMETOD BORT? DE LÄGGS SEDAN TILL  IGEN. HAR DE FEL FORMAT I MALLDATABASEN? BÄTTRE ATT ÄNDRA DET I SÅ FALL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altering fields for LG040_1_SKB_XYZ\n",
      "Altering fields for LG040_2_SKB_XYZ\n",
      "Altering fields for LG040_3_SKB_XYZ\n",
      "Altering fields for LG040_4_SKB_XYZ\n",
      "Altering fields for LG040_5_SKB_XYZ\n",
      "Altering fields for LG040_6_SKB_XYZ\n",
      "Altering fields for LG661_1_SKB_XYZ\n",
      "Altering fields for LG661_2_SKB_XYZ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import arcpy, os, glob, shutil\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#\n",
    "#SKB_XYZ_fd = os.path.join(prj_dir, gdb ,\"SKB_XYZ\")                                                                                            \n",
    "#\n",
    "#def alterfield(powerline):    \n",
    "#    LG = powerline[0]\n",
    "#    line = powerline[1]\n",
    "#    \n",
    "#    fieldnames = [\"_SKB_XYZ\"]\n",
    "#    \n",
    "#    for fieldname in fieldnames:\n",
    "#        \n",
    "#        fc_SKB = f\"{LG}_{line}{fieldname}\"\n",
    "#        SKB_path = os.path.join(SKB_XYZ_fd, f\"{LG}_{line}{fieldname}\")\n",
    "#        print(f\"Altering fields for {LG}_{line}{fieldname}\")\n",
    "#        path_joined = os.path.join(SKB_XYZ_fd, f\"{LG}_{line}{fieldname}\")\n",
    "#        field_names = [f.name for f in arcpy.ListFields(path_joined)]\n",
    "#        # Activating workspace\n",
    "#        arcpy.env.workspace = SKB_path\n",
    "#        arcpy.env.overwriteOutput = True\n",
    "#        \n",
    "#        if \"LEDNINGSGATA\" in field_names:    \n",
    "#            arcpy.management.DeleteField(\n",
    "#                path_joined,\n",
    "#                \"LEDNINGSGATA\")\n",
    "#        if \"INSAMLINGSMETOD\" in field_names:    \n",
    "#            arcpy.management.DeleteField(\n",
    "#                path_joined,\n",
    "#                \"INSAMLINGSMETOD\")\n",
    "#\n",
    "#LGs_info = pd.read_csv(LGs_info_path, sep=\"\\t\", header=0)\n",
    "#LGs_info.apply(alterfield, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fyll i Littera, Ursprung, Ursprung_Datum, Registreringsdatum\n",
    "Borde göras i ett tidigare steg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing LG022_1\n",
      "Klart\n",
      "Doing LG022_2\n",
      "Klart\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add data from LEDNIGNAR.txt\n",
    "import os\n",
    "import pandas as pd\n",
<<<<<<< Updated upstream
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "import arcpy\n",
=======
    "from psycopg2 import sql\n",
>>>>>>> Stashed changes
    "\n",
    "SKB_location = os.path.join(working_gdb, 'SKB_XYZ')\n",
    "\n",
    "date_field = 'RGDTM'\n",
    "#matosakerhet_h_field = 'MATOSAKERHET_HOJD'\n",
    "\n",
    "urspr = \"SWECO\"\n",
    "#ins_met = 20\n",
    "#matosak_plan = 1000\n",
    "#matosak_hojd = 1000\n",
    "lev_dat = str(date.today())\n",
    "\n",
    "def add_date_and_accuracy(powerline):\n",
    "    LG = powerline[\"LG\"]\n",
    "    line = powerline[\"line\"]\n",
    "    littera = powerline[\"Littera\"]\n",
    "    #LG_code = cvd_LEDNINGSGATA[LG]\n",
    "    regdatum = skanningsdatum[f\"{LG}_{line}\"]\n",
    "    \n",
    "    LG_line = f\"{LG}_{line}\"\n",
    "    print(f\"Doing {LG_line}\")\n",
    "\n",
    "    fc_SKB = f\"{LG}_{line}_SKB_XYZ\"\n",
    "    SKB_path = os.path.join(SKB_location, fc_SKB)\n",
    "    # Activating workspace\n",
    "    arcpy.env.workspace = SKB_path\n",
    "    arcpy.env.overwriteOutput = True\n",
    "    \n",
    "\n",
    "    \n",
    "    arcpy.management.CalculateField(fc_SKB, \"LITTERA\", \"\".join((\"'\", littera, \"'\")))\n",
    "    arcpy.management.CalculateField(fc_SKB, \"Ursprung\", \"\".join((\"'\", urspr, \"'\")))\n",
    "    arcpy.management.CalculateField(fc_SKB, \"Ursprung_Datum\", \"\".join((\"'\", lev_dat, \"'\")))\n",
    "    arcpy.management.CalculateField(fc_SKB, \"RGDTM\", \"\".join((\"'\", regdatum, \"'\")))\n",
    "    #sr = arcpy.SpatialReference(\"SWEREF99_TM\")\n",
    "    #arcpy.DefineProjection_management(fc_SKB, sr)\n",
    "    # Assigning domains to fields\n",
    "    arcpy.management.AssignDomainToField(\n",
    "        fc_SKB, \"LITTERA\", \"cvd_LITTERA_LEDNING\")\n",
    "    \n",
    "    print(\"Klart\")\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "\n",
    "df_skanningsdatum = pd.read_csv(scandate_file, sep=\"\\t\", header=0)\n",
    "skanningsdatum = {f\"{row['LG']}_{row['line']}\": row['skanningsdatum'] for _, row in df_skanningsdatum.iterrows()}\n",
    "\n",
    "#df_cvd_LEDNINGSGATA = pd.read_csv(cvd_LEDNINGSGATA_path, sep=\"\\t\", header=0)\n",
    "#df_cvd_DATE = pd.read_csv(cvd_DATE_path, sep=\"\\t\", header=0)\n",
    "#cvd_LEDNINGSGATA = {df_cvd_LEDNINGSGATA.LG[i]: df_cvd_LEDNINGSGATA.Code[i] for i in range(\n",
    "#    len(df_cvd_LEDNINGSGATA))}\n",
    "#cvd_DATE = {df_cvd_DATE.LG[i].join(\n",
    "#    \"_00\" + str(df_cvd_DATE.nr[i])): df_cvd_DATE.Datum[i] for i in range(len(df_cvd_DATE))}\n",
    "\n",
    "powerlines_df.apply(add_date_and_accuracy, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Ändra data i fält:\n",
    "Avrunda till en decimal, fixa rätt fältnamn och alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altering fields for LG040_1_SKB_XYZ\n",
      "No RBX for LG040_1\n",
      "Altering fields for LG040_2_SKB_XYZ\n",
      "No RBX for LG040_2\n",
      "Altering fields for LG040_3_SKB_XYZ\n",
      "No RBX for LG040_3\n",
      "Altering fields for LG040_4_SKB_XYZ\n",
      "No RBX for LG040_4\n",
      "Altering fields for LG040_5_SKB_XYZ\n",
      "No RBX for LG040_5\n",
      "Altering fields for LG040_6_SKB_XYZ\n",
      "No RBX for LG040_6\n",
      "Altering fields for LG661_1_SKB_XYZ\n",
      "No RBX for LG661_1\n",
      "Altering fields for LG661_2_SKB_XYZ\n",
      "No RBX for LG661_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import arcpy, os, glob, shutil\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#\n",
    "#def alterfield(powerline):    \n",
    "#    LG = powerline[0]\n",
    "#    line = powerline[1]\n",
    "# \n",
    "#    fieldnames = [\"_SKB_XYZ\"]\n",
    "#    \n",
    "#    for fieldname in fieldnames:\n",
    "#        fc_SKB = f\"{LG}_{line}{fieldname}\"\n",
    "#        SKB_path = os.path.join(SKB_location, f\"{LG}_{line}{fieldname}\")\n",
    "#        # Activating workspace\n",
    "#        arcpy.env.workspace = SKB_path\n",
    "#        arcpy.env.overwriteOutput = True\n",
    "#        \n",
    "#        if arcpy.Exists(fc_SKB):\n",
    "#            SKB_XYZ_fc = f\"{LG}_{line}{fieldname}\"\n",
    "#            print(f\"Altering fields for {LG}_{line}{fieldname}\")\n",
    "#            path_joined = os.path.join(SKB_XYZ_fd, f'{LG}_{line}{fieldname}')\n",
    "#            field_names = [f.name for f in arcpy.ListFields(path_joined)]\n",
    "#            if \"AVSTAND_HORISONTELLT\" in field_names: \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"AVSTAND_HORISONTELLT\",\n",
    "#                    \"AVSTAND_HORISONTELLT\",\n",
    "#                    \"AVSTAND_HORISONTELLT\")\n",
    "#\n",
    "#                arcpy.env.workspace = SKB_XYZ_fd\n",
    "#                field = ['AVSTAND_HORISONTELLT']\n",
    "#                cursor = arcpy.UpdateCursor(SKB_XYZ_fc,field)\n",
    "#\n",
    "#                with arcpy.da.UpdateCursor(SKB_XYZ_fc,field) as cursor:\n",
    "#                    for row in cursor:\n",
    "#                        row[0] = float(\"{:.1f}\".format(round(row[0], 1)))\n",
    "#                        cursor.updateRow(row)\n",
    "#\n",
    "#            if \"AVSTAND_FAS\" in field_names:    \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"AVSTAND_FAS\",\n",
    "#                    \"AVSTAND_FAS\",\n",
    "#                    \"AVSTAND_FAS\")\n",
    "#\n",
    "#                arcpy.env.workspace = SKB_XYZ_fd\n",
    "#                field = ['AVSTAND_FAS']\n",
    "#                cursor = arcpy.UpdateCursor(SKB_XYZ_fc,field)\n",
    "#\n",
    "#                with arcpy.da.UpdateCursor(SKB_XYZ_fc,field) as cursor:\n",
    "#                    for row in cursor:\n",
    "#                        row[0] = float(\"{:.1f}\".format(round(row[0], 1)))\n",
    "#                        cursor.updateRow(row)\n",
    "#\n",
    "#            if \"DELTA_HOJD\" in field_names:    \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"DELTA_HOJD\",\n",
    "#                    \"DELTA_HOJD\",\n",
    "#                    \"DELTA_HOJD\")\n",
    "#\n",
    "#                arcpy.env.workspace = SKB_XYZ_fd\n",
    "#                field = ['DELTA_HOJD']\n",
    "#                cursor = arcpy.UpdateCursor(SKB_XYZ_fc,field)\n",
    "#\n",
    "#                with arcpy.da.UpdateCursor(SKB_XYZ_fc,field) as cursor:\n",
    "#                    for row in cursor:\n",
    "#                        row[0] = float(\"{:.1f}\".format(round(row[0], 1)))\n",
    "#                        cursor.updateRow(row)\n",
    "#\n",
    "#            if \"RGDTM\" in field_names: \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"RGDTM\",\n",
    "#                    \"RGDTM\",\n",
    "#                    \"RGDTM\")\n",
    "#\n",
    "#            if \"MATOSAKERHET_PLAN\" in field_names: \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"MATOSAKERHET_PLAN\",\n",
    "#                    \"Matosakerhet_Plan\",\n",
    "#                    \"Matosakerhet_Plan\")\n",
    "#\n",
    "#            if \"MATOSAKERHET_HOJD\" in field_names: \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"MATOSAKERHET_HOJD\",\n",
    "#                    \"Matosakerhet_Hojd\",\n",
    "#                    \"Matosakerhet_Hojd\")\n",
    "#\n",
    "#            if \"LEDNINGSGATA\" in field_names: \n",
    "#                arcpy.management.AlterField(\n",
    "#                    path_joined,\n",
    "#                    \"LEDNINGSGATA\",\n",
    "#                    \"LEDNINGSGATA\",\n",
    "#                    \"LEDNINGSGATA\")\n",
    "#\n",
    "#\n",
    "#    else:\n",
    "#        print(f\"No RBX for {LG}_{line}\")\n",
    "#    \n",
    "#LGs_info = pd.read_csv(LGs_info_path, sep=\"\\t\", header=0)\n",
    "#LGs_info.apply(alterfield, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slå ihop kantträd för alla ledningar till en featureclass\n",
    "## jag har börjat att göra det manuellt /Karin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LG040_1 to list\n",
      "Done adding LG040_1 to list\n",
      "Adding LG040_2 to list\n",
      "Done adding LG040_2 to list\n",
      "Adding LG040_3 to list\n",
      "Done adding LG040_3 to list\n",
      "Adding LG040_4 to list\n",
      "Done adding LG040_4 to list\n",
      "Adding LG040_5 to list\n",
      "Done adding LG040_5 to list\n",
      "Adding LG040_6 to list\n",
      "Done adding LG040_6 to list\n",
      "Adding LG661_1 to list\n",
      "Done adding LG661_1 to list\n",
      "Adding LG661_2 to list\n",
      "Done adding LG661_2 to list\n",
      "Doing merge\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: den 1 november 2023 16:29:41\",\"Succeeded at den 1 november 2023 16:29:47 (Elapsed Time: 5,98 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\semnve\\\\Desktop\\\\RBX_och_SKB\\\\SKB_2023_11_01\\\\SKB_2023_11_01.gdb\\\\all\\\\all_2023_11_01'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Merge all to one\n",
    "#import datetime, arcpy, os\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#\n",
    "#all_location = os.path.join(gdb, 'SKB_XYZ')\n",
    "#all_merged_location = os.path.join(gdb, 'all')\n",
    "#all_merged_path = os.path.join(all_merged_location, all_merged_name)\n",
    "#\n",
    "#def to_all_list(powerline):\n",
    "#    LG = powerline[0]\n",
    "#    line = powerline[1]\n",
    "#    \n",
    "#    print(f\"Adding {LG}_{line} to list\")\n",
    "#    \n",
    "#    all_path = os.path.join(all_location, f\"{LG}_{line}_SKB_XYZ\")\n",
    "#\n",
    "#    all_list.append(all_path)\n",
    "#    \n",
    "#    print(f\"Done adding {LG}_{line} to list\")\n",
    "#\n",
    "#    \n",
    "#all_list = []\n",
    "#\n",
    "#LGs_info = pd.read_csv(LGs_info_path, sep=\"\\t\", header=0)\n",
    "#LGs_info.apply(to_all_list, axis=1)\n",
    "#\n",
    "#print(\"Doing merge\")\n",
    "#arcpy.Merge_management(all_list, all_merged_path)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klipp träffpunkter\n",
    "Se till att använda aktuella data i LG_polygons och station_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages'>Start Time: den 29 januari 2025 09:05:51<br>Succeeded at den 29 januari 2025 09:05:52 (Elapsed Time: 0,64 seconds)<br></div><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'true'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#fc_ledningsgator = r\"Q:\\Projekt\\Data_2024\\Underlag_SVK\\Bestallningsunderlag_2024.gdb\\GNG_LEDNINGSGATA\" #os.path.join(gdb, \"Gator2021\")\n",
    "#fc_stationsomraden = r\"Q:\\Projekt\\Data_2024\\Underlag_SVK\\Bestallningsunderlag_2024.gdb\\GNG_STATIONSOMRADE\" #os.path.join(gdb, \"STATIONSOMRADE\")\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "##fd_ej_akuta = os.path.join(gdb, \"ej_akuta\")\n",
    "#fd_all = os.path.join(gdb, \"all\")\n",
    "#fd_traffpunkter = os.path.join(gdb, \"traffpunkter\")\n",
    "\n",
    "#fc_all = os.path.join(fd_all, f\"all_{version}\")\n",
    "#fc_traffpunkter = os.path.join(fd_traffpunkter, f\"traffpunkter_{version}\")\n",
    "tmp_layer_traffpunkter = os.path.join(working_gdb, \"tmp_layer_traffpunkter\")\n",
    "fc_traffpunkter = os.path.join(results_gdb, 'kantträd')\n",
    "fc_all = os.path.join(working_gdb, \"kantträd_oklippta\")\n",
    "arcpy.management.Delete(tmp_layer_traffpunkter)\n",
    "\n",
    "# Gör kopia av oklippta\n",
    "arcpy.management.CopyFeatures(fc_all, fc_traffpunkter)\n",
    "\n",
    "arcpy.management.MakeFeatureLayer(fc_traffpunkter, tmp_layer_traffpunkter)\n",
    "\n",
    "\n",
    "# Steg 1\n",
    "# Inom fastbredd och stationsområden: ta bort alla inom 7 m horisontellt avstånd från fas\n",
    "#\n",
    "# 1.1 Välj alla inom fastbredd\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = LG_polygons,\n",
    "                                       selection_type = \"NEW_SELECTION\")\n",
    "# 1.2 Lägg till alla inom stationsområden\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = station_polygons,\n",
    "                                       selection_type = \"ADD_TO_SELECTION\")\n",
    "# 1.3 Av de valda, behåll punkter med avst_hori < 7 m\n",
    "arcpy.management.SelectLayerByAttribute(in_layer_or_view = tmp_layer_traffpunkter,\n",
    "                                        selection_type = \"SUBSET_SELECTION\", \n",
    "                                        where_clause = '\"AVSTAND_HORISONTELLT\" < 7')\n",
    "# 1.4 Radera valda\n",
    "arcpy.management.DeleteFeatures(tmp_layer_traffpunkter)\n",
    "\n",
    "# Steg 2\n",
    "# Inom fastbredd och stationsområden: ta bort alla med trädhöjd <= 4.5 m och avst_fas >= 5 m\n",
    "#\n",
    "# 2.1 Välj alla inom fastbredd\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = LG_polygons,\n",
    "                                       selection_type = \"NEW_SELECTION\")\n",
    "# 2.2 Lägg till alla inom stationsområden\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = station_polygons,\n",
    "                                       selection_type = \"ADD_TO_SELECTION\")\n",
    "# 2.3 Av de valda, gör gör subset selection med punkter med trädhöjd <= 4.5 OCH avst_fas >= 5\n",
    "arcpy.management.SelectLayerByAttribute(in_layer_or_view = tmp_layer_traffpunkter,\n",
    "                                       selection_type = \"SUBSET_SELECTION\",\n",
    "                                       where_clause = '\"DELTA_HOJD\" <= 4.5 AND \"AVSTAND_FAS\" >= 5')\n",
    "# 2.4 Radera valda\n",
    "arcpy.management.DeleteFeatures(tmp_layer_traffpunkter)\n",
    "\n",
    "## Kopiera kvarvarande punkter till traffpunkter\n",
    "#arcpy.management.CopyFeatures(tmp_layer_traffpunkter, fc_traffpunkter) # Överflödigt?\n",
    "\n",
    "# Ta bort det temporära lagret\n",
    "arcpy.management.Delete(tmp_layer_traffpunkter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Rensning onödig information\n",
    "Rensar rådata från onödig information\n",
    "\n",
    "Första cellen gör rensningen på de ej hopslagna featureklasserna dvs varje ledninge för sig\n",
    "\n",
    "Andra cellen gör rensningen på featureklassen med alla ledningar hopslagna (hopslagningen görs under rubrik \"Slå ihop kantträd för alla ledningar till en featureklass\" ovan)\n",
    "\n",
    "TODO: gör rensningen på enskilda ledningar innan hopslagningen görs. Kontrollera om rensningen även kan göras innan klippningen, eller om klippningen är beroende av några av de fält som här rensas bort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altering fields for LG040_1_SKB_XYZ\n",
      "No RBX for LG040_1\n",
      "Altering fields for LG040_2_SKB_XYZ\n",
      "No RBX for LG040_2\n",
      "Altering fields for LG040_3_SKB_XYZ\n",
      "No RBX for LG040_3\n",
      "Altering fields for LG040_4_SKB_XYZ\n",
      "No RBX for LG040_4\n",
      "Altering fields for LG040_5_SKB_XYZ\n",
      "No RBX for LG040_5\n",
      "Altering fields for LG040_6_SKB_XYZ\n",
      "No RBX for LG040_6\n",
      "Altering fields for LG661_1_SKB_XYZ\n",
      "No RBX for LG661_1\n",
      "Altering fields for LG661_2_SKB_XYZ\n",
      "No RBX for LG661_2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import arcpy, os, glob, shutil\n",
    "#from pathlib import Path\n",
    "#import pandas as pd\n",
    "#\n",
    "#\n",
    "#SKB_XYZ_fd = os.path.join(prj_dir, gdb, \"SKB_XYZ\")                                                                                            \n",
    "#\n",
    "#def alterfield(powerline):    \n",
    "#    LG = powerline[0]\n",
    "#    line = powerline[1]\n",
    "#    \n",
    "#    fieldnames = [\"_SKB_XYZ\"]\n",
    "#    \n",
    "#    for fieldname in fieldnames:\n",
    "#        fc_SKB = f\"{LG}_{line}{fieldname}\"\n",
    "#        SKB_path = os.path.join(SKB_XYZ_fd, fc_SKB)\n",
    "#        # Activating workspace\n",
    "#        arcpy.env.workspace = SKB_path\n",
    "#        arcpy.env.overwriteOutput = True\n",
    "#        \n",
    "#        \n",
    "#        if arcpy.Exists(fc_SKB):\n",
    "#            SKB_XYZ_fc = f\"{LG}_{line}{fieldname}\"\n",
    "#            print(f\"Altering fields for {LG}_{line}{fieldname}\")\n",
    "#            path_joined = os.path.join(SKB_XYZ_fd, f'{LG}_{line}{fieldname}')\n",
    "#            field_names = [f.name for f in arcpy.ListFields(path_joined)]\n",
    "#\n",
    "#            if \"GIS_ID\" in field_names:    \n",
    "#                    arcpy.management.DeleteField(\n",
    "#                        path_joined,\n",
    "#                        \"GIS_ID\")\n",
    "#            if \"MAX_TILLVAXT\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"MAX_TILLVAXT\")\n",
    "#            if \"created_user\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"created_user\")\n",
    "#            if \"created_date\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"created_date\")\n",
    "#            if \"last_edited_user\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"last_edited_user\")\n",
    "#            if \"last_edited_date\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"last_edited_date\")\n",
    "#            if \"ATGARDSAR\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"ATGARDSAR\")\n",
    "#            if \"LEVERANTOR\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"LEVERANTOR\")\n",
    "#            if \"LEVERANTORS_DATUM\" in field_names:    \n",
    "#                arcpy.management.DeleteField(\n",
    "#                    path_joined,\n",
    "#                    \"LEVERANTORS_DATUM\")\n",
    "#    else:\n",
    "#        print(f\"No RBX for {LG}_{line}\")\n",
    "#    \n",
    "#LGs_info = pd.read_csv(LGs_info_path, sep=\"\\t\", header=0)\n",
    "#LGs_info.apply(alterfield, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n",
      "all\n",
      "C:\\Users\\semnve\\Desktop\\RBX_och_SKB\\SKB_2023_11_01\\SKB_2023_11_01.gdb\\all\n",
      "Altering fields for all_2023_11_01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    None\n",
       "1    None\n",
       "2    None\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rensar rådata från onödig information\n",
    "\n",
    "import arcpy, os, glob, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def alterfield(powerline):    \n",
    "    LG = powerline[0]\n",
    "    line = powerline[1]\n",
    "    \n",
    "    datasetnames = [\"all\"]\n",
    "    fieldnames = [version]\n",
    "    \n",
    "    for dataset in datasetnames:\n",
    "        print(dataset)\n",
    "        for fieldname in fieldnames:\n",
    "            fc_SKB = f\"{dataset}_{fieldname}\"\n",
    "            if \"all\" in dataset:\n",
    "                SKB_location = os.path.join(prj_dir,gdb,f\"{dataset}\")\n",
    "            else:\n",
    "                SKB_location = os.path.join(prj_dir,gdb,f\"{dataset}\")\n",
    "            print(SKB_location)\n",
    "            SKB_path = os.path.join(SKB_location, fc_SKB)\n",
    " \n",
    "            # Activating workspace\n",
    "            arcpy.env.workspace = SKB_path\n",
    "            arcpy.env.overwriteOutput = True\n",
    "\n",
    "            if arcpy.Exists(fc_SKB):\n",
    "                SKB_XYZ_fc = fc_SKB\n",
    "                print(f\"Altering fields for {dataset}_{fieldname}\")\n",
    "                path_joined = os.path.join(SKB_location, f'{dataset}_{fieldname}')\n",
    "                field_names = [f.name for f in arcpy.ListFields(SKB_path)]\n",
    "\n",
    "                if \"GIS_ID\" in field_names:    \n",
    "                        arcpy.management.DeleteField(\n",
    "                            path_joined,\n",
    "                            \"GIS_ID\")\n",
    "                if \"MAX_TILLVAXT\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"MAX_TILLVAXT\")\n",
    "                if \"created_user\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"created_user\")\n",
    "                if \"created_date\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"created_date\")\n",
    "                if \"last_edited_user\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"last_edited_user\")\n",
    "                if \"last_edited_date\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"last_edited_date\")\n",
    "                if \"ATGARDSAR\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"ATGARDSAR\")\n",
    "                if \"LEVERANTOR\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"LEVERANTOR\")\n",
    "                if \"LEVERANTORS_DATUM\" in field_names:    \n",
    "                    arcpy.management.DeleteField(\n",
    "                        path_joined,\n",
    "                        \"LEVERANTORS_DATUM\")\n",
    "                \n",
    "                dataset = SKB_location\n",
    "                sr = arcpy.SpatialReference(\"SWEREF99_TM\")\n",
    "                arcpy.DefineProjection_management(dataset, sr)\n",
    "                # Assigning domains to fields\n",
    "                arcpy.management.AssignDomainToField(\n",
    "                    fc_SKB, \"LEDNINGSGATA\", \"cvd_LEDNINGSGATA\")\n",
    "                arcpy.management.AssignDomainToField(\n",
    "                    fc_SKB, \"LITTERA\", \"cvd_LITTERA_LEDNING\")\n",
    "                arcpy.management.AssignDomainToField(\n",
    "                    fc_SKB, \"Insamlingsmetod\", \"cvd_INMATNINGSMETOD\")\n",
    "                arcpy.management.AssignDomainToField(\n",
    "                    fc_SKB, \"Matosakerhet_Plan\", \"cvd_MATOSAKERHET\")\n",
    "                arcpy.management.AssignDomainToField(\n",
    "                    fc_SKB, \"Matosakerhet_Hojd\", \"cvd_MATOSAKERHET\")\n",
    "    \n",
    "LGs_info = pd.read_csv(LGs_info_path, sep=\"\\t\", header=0)\n",
    "LGs_info.apply(alterfield, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      LEDNINGSGATA    LITTERA\n",
      "0              100   UL6 S3-4\n",
      "4              100   UL6 S5-7\n",
      "12             230   UL5 S1-5\n",
      "13             280     AL1 S2\n",
      "16             280     AL1 S7\n",
      "19             320   AL5 S2-3\n",
      "17             320     AL6 S1\n",
      "20             330        KL4\n",
      "25             330     RL3 S1\n",
      "27             330     RL3 S3\n",
      "66             340     AL5 S1\n",
      "350            470     RL2 S1\n",
      "352            480     CL3 S0\n",
      "353            480     CL3 S2\n",
      "363            480   CL5 S1-2\n",
      "369            530  RL22 S4-5\n",
      "512            530  RL22 S6-7\n",
      "828            540    UL17 S4\n",
      "864            550     CL3 S2\n",
      "1302           550     RL8 S7\n",
      "1416           570     RL5 S3\n",
      "1453           570     RL5 S4\n",
      "1463           580   CL5 S1-2\n"
     ]
    }
   ],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "gdb_path = r\"C:\\SVK_2024\\pythonkörningar\\results_2501xx.gdb\"\n",
    "feature_class = \"RBX_closest_points\"\n",
    "\n",
    "gdf = gpd.read_file(gdb_path, layer=feature_class)\n",
    "\n",
    "gdf = gdf[[\"LEDNINGSGATA\", \"LITTERA\"]]\n",
    "unique_df = gdf.drop_duplicates().sort_values([\"LEDNINGSGATA\", \"LITTERA\"])\n",
    "\n",
    "print(unique_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal featureklasser: 27\n",
      "119140\n"
     ]
    }
   ],
   "source": [
    "arcpy.env.workspace = os.path.join(working_gdb, 'SKB_XYZ')\n",
    "\n",
    "feature_classes = arcpy.ListFeatureClasses()\n",
    "print(f\"Antal featureklasser: {len(feature_classes)}\")\n",
    "\n",
    "antal_kantträd = 0\n",
    "for fc in feature_classes:\n",
    "    feature_count = int(arcpy.management.GetCount(fc)[0])\n",
    "    antal_kantträd = antal_kantträd + feature_count\n",
    "\n",
    "print(antal_kantträd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/SVK_2024/pythonkörningar\\working_2501yy.gdb\n"
     ]
    }
   ],
   "source": [
    "print(working_gdb)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted PostGIS kantträd into gdb C:/Uppdrag/2024/SVK_2024/pythonkörningar\\results_250425.gdb.\n"
     ]
    }
   ],
   "source": [
    "# TODO check if shape should be mz or z, currently mz? \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "def postGIS_to_gdb():\n",
    "    table_name = \"kantträd\"\n",
    "\n",
    "    ogr_command = [\n",
    "        ogr2ogr_path,\n",
    "        \"-f\", \"OpenFileGDB\",\n",
    "        results_gdb,      \n",
    "        pg_connection,\n",
    "        \"-a_srs\", \"EPSG:3006\",\n",
    "        \"-nlt\", \"POINTZ\",\n",
    "        \"-dim\", \"3\",\n",
    "        \"-append\",\n",
    "        \"-sql\", f\"SELECT * FROM public.{table_name}\",\n",
    "        \"-nln\", table_name,\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ogr_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully converted PostGIS {table_name} into gdb {results_gdb}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during import: {e}\")\n",
    "\n",
    "try:\n",
    "    pg_connection = f\"PG:host={HOST} dbname={DB_NAME} user={USER} password={PASSWORD} port={PORT}\"\n",
    "    ogr2ogr_path = ogr2ogr_path\n",
    "    os.environ[\"PROJ_LIB\"] = proj_lib_path #TODO potentially solve this in another way, but needed to specify EPSG:3006\n",
    "    os.environ[\"GDAL_DATA\"] = gdal_data_path\n",
    "\n",
    "    postGIS_to_gdb()\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)"
   ]
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
