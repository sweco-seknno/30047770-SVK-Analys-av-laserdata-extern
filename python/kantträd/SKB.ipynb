{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Trees Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "#settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEKNNO.json\"\n",
    "#settings_file = r\"Q:\\Projekt\\Data_2024\\styrfiler\\settings_SEVPLI.json\"\n",
    "#settings_file = r\"C:\\SVK_utveckling\\settings_SEVPLI.json\"\n",
    "settings_file = r\"C:\\Users\\SE1K4H\\Desktop\\SVK-Analys-Filer\\settings_SEKNNO.json\" # Working on Elsas's computer\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(settings_file):\n",
    "        raise FileNotFoundError(f\"Could not find {settings_file}\")\n",
    "\n",
    "    with open(settings_file, 'r', encoding='utf-8') as file:\n",
    "        settings = json.load(file)\n",
    "\n",
    "    # TODO: check which ones are needed and not\n",
    "    run_ID = settings[\"run_ID\"]\n",
    "    powerline_list = settings[\"powerline_list\"]\n",
    "    local_dir = settings[\"local_folder\"]\n",
    "    working_gdb_template = settings[\"working_gdb_template\"]\n",
    "    wires_gdb_template = settings[\"wires_gdb_template\"]\n",
    "    domains_folder = settings[\"domains_folder\"]\n",
    "    scandate_file = settings[\"scandate_file\"]\n",
    "    cvd_LEDNINGSGATA_path = os.path.join(domains_folder, \"cvd_LEDNINGSGATA.txt\")\n",
    "    powerlines_folder = settings[\"powerlines_folder\"]\n",
    "    LG_polygons = settings[\"LG_polygons\"]\n",
    "    station_polygons = settings[\"station_polygons\"]\n",
    "    module_path = settings[\"modules\"] \n",
    "    ogr2ogr_path = settings[\"ogr2ogr_path\"]   \n",
    "    proj_lib_path = settings[\"proj_lib_path\"]\n",
    "    gdal_data_path = settings[\"gdal_data_path\"]\n",
    "    DEFAULT_DB_NAME = settings[\"default_db_name\"]\n",
    "    DB_NAME = settings[\"db_name\"]\n",
    "    USER = settings[\"db_user\"]\n",
    "    PASSWORD = settings[\"db_password\"]\n",
    "    HOST = settings[\"host\"]\n",
    "    PORT = settings[\"port\"] \n",
    "\n",
    "    if None in (run_ID, powerline_list, local_dir, working_gdb_template, wires_gdb_template, domains_folder, scandate_file, powerlines_folder, LG_polygons, station_polygons, module_path, ogr2ogr_path, proj_lib_path, gdal_data_path, DEFAULT_DB_NAME, DB_NAME, USER, PASSWORD, HOST, PORT):\n",
    "        raise KeyError(f\"One or more keys are missing in {settings_file}\")\n",
    "    \n",
    "    working_gdb = os.path.join(local_dir, f\"working_{run_ID}_PostGIS.gdb\")\n",
    "    \n",
    "    print(f\"Settings loaded successfully.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Invalid JSON format in {settings_file}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PostGIS methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "\n",
    "# TODO can move these kind of methods to utils or similiar?\n",
    "def connect_to_database(db_name):\n",
    "    try:\n",
    "        conn = psycopg.connect(dbname=db_name, user=USER, password=PASSWORD, host=HOST, port=PORT)\n",
    "        conn.autocommit = True\n",
    "        return conn\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"Error connecting to {db_name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def execute_query(conn, query, data=None, fetch=False):\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query, data or ())\n",
    "            if fetch:\n",
    "                return cur.fetchall()\n",
    "    except psycopg.Error as e:\n",
    "        print(f\"Error executing query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy template gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/SE1K4H/Desktop/SVK-Analys-Filer/pythonkörningar/test_postGIS\\working_250201_PostGIS.gdb already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(working_gdb):\n",
    "    print(f\"{working_gdb} already exists\")\n",
    "else: \n",
    "    shutil.copytree(working_gdb_template, working_gdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge SKB text files for all blocks of a powerline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged all edge trees blocks for LG001/line_1 into text file C:\\Users\\SE1K4H\\Desktop\\SVK-Analys-Filer\\ledningar\\LG001\\line_1\\kantträd\\SKB_raw.txt.\n",
      "Done with all power lines\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import fileinput\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# TODO add exception handling.\n",
    "\n",
    "def combine_blocks(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    \n",
    "    block_dir = os.path.join(line_dir, \"kantträd\", \"block\")\n",
    "    combined_blocks_path = os.path.join(line_dir, \"kantträd\", \"SKB_raw.txt\")\n",
    "    \n",
    "    # Merge files for all blocks into one\n",
    "    merge_blocks(block_dir, \"*.txt\", combined_blocks_path)\n",
    "    print(f\"Successfully merged all edge trees blocks for {LG}/line_{line} into text file {combined_blocks_path}.\")\n",
    "\n",
    "def merge_blocks(src_dir, search_pattern, dst_file):\n",
    "    blocks = glob.glob(os.path.join(src_dir, search_pattern))\n",
    "    with open(dst_file, \"w\") as fh:\n",
    "        input_lines = fileinput.input(blocks)\n",
    "        fh.writelines(input_lines)\n",
    "\n",
    "powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "powerlines_df.apply(combine_blocks, axis=1)\n",
    "print(f\"Done with all power lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Edge Trees to PostGIS Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted edge trees to PostGIS database.\n",
      "Connection to database test_postgis_db closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from psycopg import sql #TODO update to and install psycopg, the latest version.\n",
    "\n",
    "cur = None\n",
    "conn = None\n",
    "\n",
    "def create_SKB_XYmZ_table(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    line_dir = Path(powerlines_folder) / LG / f\"line_{line}\"\n",
    "    SKB_dir = line_dir / \"kantträd\"\n",
    "    SKB_file_in = os.path.join(SKB_dir, f\"SKB_raw.txt\")\n",
    "    drop_table_if_exists_query = sql.SQL(\"DROP TABLE IF EXISTS {table_name}\").format(table_name=sql.Identifier(table_name))\n",
    "    create_query = sql.SQL(\"CREATE TABLE {table_name}(objectid SERIAL PRIMARY KEY, x DOUBLE PRECISION, y DOUBLE PRECISION, z FLOAT, dz FLOAT, mz FLOAT, shape geometry(POINTZ, 3006))\").format(table_name=sql.Identifier(table_name)) #TODO potentially change to 5845\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create table for powerline\n",
    "        execute_query(conn_db, drop_table_if_exists_query)\n",
    "        execute_query(conn_db, create_query)\n",
    "        \n",
    "        # Step 2: Insert edge trees into powerline table \n",
    "        with open(SKB_file_in, 'r') as src_file:\n",
    "            for file_line in src_file:\n",
    "                l_split = file_line.split(' ')\n",
    "                x = float(l_split[0])\n",
    "                y = float(l_split[1])\n",
    "                z = float(l_split[2])\n",
    "                dz = float(l_split[3])\n",
    "                mz = z - dz\n",
    "                \n",
    "                #TODO potentially change to 5845, also check if SRID is needed on table level as well\n",
    "                insert_query = sql.SQL(\"\"\"\n",
    "                    INSERT INTO {table_name} (x, y, z, dz, mz, shape)\n",
    "                    VALUES (%s, %s, %s, %s, %s, ST_GeomFromText(%s, 3006))\n",
    "                \"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "                pointz_string = f\"POINTZ({x} {y} {mz})\"\n",
    "                tree_data = (x, y, z, dz, mz, pointz_string)\n",
    "                execute_query(conn_db, insert_query, tree_data)\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(create_SKB_XYmZ_table, axis=1)\n",
    "    \n",
    "    print(\"Successfully inserted edge trees to PostGIS database.\") \n",
    "\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns were added successfully to table lg001_1_skb_xymz\n",
      "avst_mz_fas was calculated succesfully for LG001_1\n",
      "avst_fas was calculated succesfully for LG001_1\n",
      "avst_hori was calculated succesfully for LG001_1\n",
      "Successfully calculated distances for all wires.\n",
      "Connection to database test_postgis_db closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_distances(row, conn_db):\n",
    "    #TODO se över SQL query om de kan förbättras, optimeras\n",
    "    #TODO Borde ta bort columner först om de existerar \n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "    wire_table_name = f\"{LG.lower()}_{line}_fas\"\n",
    "    \n",
    "    add_columns_query = sql.SQL(\"\"\"ALTER TABLE {table_name}\n",
    "        ADD COLUMN IF NOT EXISTS avst_mz_fas DOUBLE PRECISION,\n",
    "        ADD COLUMN IF NOT EXISTS avst_hori DOUBLE PRECISION, \n",
    "        ADD COLUMN IF NOT EXISTS avst_fas DOUBLE PRECISION;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "    \n",
    "    avst_mz_fas_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_mz_fas = subquery.distance\n",
    "        FROM (\n",
    "            SELECT DISTINCT ON (p.objectid)  \n",
    "                p.objectid,\n",
    "                ST_3DDistance(p.shape, l.shape) AS distance\n",
    "            FROM \n",
    "                {table_name} p, \n",
    "                {wire_table_name} l\n",
    "            WHERE \n",
    "                ST_3DDWithin(p.shape, l.shape, 100)\n",
    "            ORDER BY \n",
    "                p.objectid, distance ASC\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name), wire_table_name=sql.Identifier(wire_table_name))\n",
    "\n",
    "    avst_fas_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_fas = subquery.distance\n",
    "        FROM (\n",
    "            SELECT objectid, avst_mz_fas - dz AS distance\n",
    "            FROM {table_name}\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    avst_hori_query = sql.SQL(\"\"\"UPDATE {table_name} p\n",
    "        SET avst_hori = subquery.distance\n",
    "        FROM (\n",
    "            SELECT DISTINCT ON (p.objectid)  \n",
    "                p.objectid,\n",
    "                ST_Distance(p.shape, l.shape) AS distance\n",
    "            FROM \n",
    "                {table_name} p, \n",
    "                {wire_table_name} l\n",
    "            WHERE \n",
    "                ST_DWithin(p.shape, l.shape, 100)\n",
    "            ORDER BY \n",
    "                p.objectid, distance ASC\n",
    "        ) AS subquery\n",
    "        WHERE p.objectid = subquery.objectid;\"\"\").format(table_name=sql.Identifier(table_name), wire_table_name=sql.Identifier(wire_table_name))\n",
    "\n",
    "    # Step 1: Add attributes\n",
    "    execute_query(conn_db, add_columns_query)\n",
    "    print(f\"Columns were added successfully to table {table_name}\")\n",
    "\n",
    "    # Step 2: Calculate shortest distance between mz and phase.\n",
    "    execute_query(conn_db, avst_mz_fas_query)\n",
    "    print(f\"avst_mz_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 3: Calculate the shortest distance between tree top (z) and phase with event of potential fall.\n",
    "    execute_query(conn_db, avst_fas_query)\n",
    "    print(f\"avst_fas was calculated succesfully for {LG}_{line}\")\n",
    "\n",
    "    # Step 4: Calculate the horizontal distance between tree and phase.\n",
    "    execute_query(conn_db, avst_hori_query)\n",
    "    print(f\"avst_hori was calculated succesfully for {LG}_{line}\") \n",
    "\n",
    "try: \n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(lambda row: calculate_distances(row, conn_db), axis=1)\n",
    "    \n",
    "    print(\"Successfully calculated distances for all wires.\") #TODO printed on error, change this\n",
    "except psycopg.Error as e:\n",
    "    print(\"Error while working with PostgreSQL:\", e) \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Littera, Ursprung, Ursprung_Datum, Registreringsdatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns were added successfully to table lg001_1_skb_xymz\n",
      "Values were updated successfully in table lg001_1_skb_xymz\n",
      "Successfully inserted edge trees to PostGIS database.\n",
      "Connection to database test_postgis_db closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg\n",
    "import pandas as pd\n",
    "from psycopg import sql\n",
    "from datetime import date\n",
    "\n",
    "def add_data(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]\n",
    "    littera = row[\"Littera\"]\n",
    "    regdatum = skanningsdatum[f\"{LG}_{line}.0\"]   \n",
    "    table_name = f\"{LG.lower()}_{line}_skb_xymz\"\n",
    "\n",
    "    #TODO ledningsgata, insamlingsmetod, matosakerhet x2 should be updated to domains, using foreign keys \n",
    "    add_columns_query = sql.SQL(\"\"\"ALTER TABLE {table_name}\n",
    "                                    ADD COLUMN IF NOT EXISTS rgdtm DATE, \n",
    "                                    ADD COLUMN IF NOT EXISTS ledningsgata FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS insamlingsmetod FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS matosakerhet_plan FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS matosakerhet_hojd FLOAT,\n",
    "                                    ADD COLUMN IF NOT EXISTS littera VARCHAR(255),\n",
    "                                    ADD COLUMN IF NOT EXISTS ursprung VARCHAR(255),\n",
    "                                    ADD COLUMN IF NOT EXISTS ursprung_datum DATE;\"\"\").format(table_name=sql.Identifier(table_name))\n",
    "\n",
    "    # TODO Update littera to work for domain\n",
    "    # TODO Investigate if % or placeholder should be used\n",
    "    update_query = sql.SQL(\"\"\"UPDATE {table_name}\n",
    "                                SET rgdtm = {regdatum}, \n",
    "                                littera = {littera},\n",
    "                                ursprung = {ursprung},\n",
    "                                ursprung_datum = {lev_dat};\"\"\").format(table_name=sql.Identifier(table_name), \n",
    "                                                                       regdatum=sql.Placeholder(\"regdatum\"), \n",
    "                                                                       littera=sql.Placeholder(\"littera\"), \n",
    "                                                                       ursprung=sql.Placeholder(\"ursprung\"), \n",
    "                                                                       lev_dat=sql.Placeholder(\"lev_dat\"))\n",
    "    updated_values = {\"regdatum\": regdatum, \"littera\": littera, \"ursprung\": urspr, \"lev_dat\": lev_dat}\n",
    "\n",
    "    try:\n",
    "        # Step 1: Add columns\n",
    "        execute_query(conn_db, add_columns_query)\n",
    "        print(f\"Columns were added successfully to table {table_name}\")\n",
    "        \n",
    "        # Step 2: Update Littera, Ursprung, Ursprung_Datum, Registreringsdatum \n",
    "        execute_query(conn_db, update_query, updated_values)\n",
    "        print(f\"Values were updated successfully in table {table_name}\")\n",
    "\n",
    "    except psycopg.Error as e:\n",
    "        print(\"Error while working with PostgreSQL:\", e)   \n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    df_skanningsdatum = pd.read_csv(scandate_file, sep=\"\\t\", header=0) #TODO english variables?\n",
    "    skanningsdatum = {f\"{row['LG']}_{row['line']}\": row['skanningsdatum'] for _, row in df_skanningsdatum.iterrows()}\n",
    "    urspr = \"SWECO\"\n",
    "    lev_dat = str(date.today())\n",
    "\n",
    "    if conn_db:\n",
    "        powerlines_df.apply(add_data, axis=1) #TODO don't have to pass conn_db on the other places, do like this instead\n",
    "    \n",
    "    print(\"Successfully inserted edge trees to PostGIS database.\") \n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge edge trees for all powerlines into one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found tables: ['lg001_1_skb_xymz', 'lg001_2_skb_xymz', 'lg001_3_skb_xymz']\n",
      "Successfully merged all edge trees tables into one table.\n",
      "Connection to database test_postgis_db closed.\n"
     ]
    }
   ],
   "source": [
    "def list_skb_xymz_tables():\n",
    "    select_tables_query = sql.SQL(\"\"\"\n",
    "        SELECT table_name\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_name LIKE '%%skb_xymz' AND table_schema = 'public';\n",
    "    \"\"\")\n",
    "\n",
    "    result = execute_query(conn_db, select_tables_query, fetch=True)\n",
    "    table_names = [t[0] for t in result]\n",
    "\n",
    "    return table_names\n",
    "\n",
    "def merge_tables(table_names):\n",
    "    table_name = 'kantträd_oklippta'\n",
    "\n",
    "    create_table_query = sql.SQL(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} AS\n",
    "        TABLE {first_table} WITH NO DATA;\n",
    "    \"\"\").format(table_name=sql.Identifier(table_name), first_table=sql.Identifier(table_names[0]))\n",
    "\n",
    "    execute_query(conn_db, create_table_query)\n",
    "\n",
    "    for table in table_names:\n",
    "        insert_query = sql.SQL(\"\"\"\n",
    "            INSERT INTO {table_name}\n",
    "            SELECT * FROM {table}\n",
    "        \"\"\").format(table_name=sql.Identifier(table_name), table=sql.Identifier(table))\n",
    "\n",
    "        execute_query(conn_db, insert_query)\n",
    "\n",
    "try: \n",
    "    conn_db = connect_to_database(DB_NAME)\n",
    "\n",
    "    if conn_db:\n",
    "        table_names = list_skb_xymz_tables()\n",
    "        if table_names:\n",
    "            print(f\"Found tables: {table_names}\")\n",
    "            merge_tables(table_names)\n",
    "            print(\"Successfully merged all edge trees tables into one table.\") \n",
    "        else:\n",
    "            print(\"No tables ending with 'skb_xymz' found.\")\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)\n",
    "finally: \n",
    "    if conn_db is not None:\n",
    "        conn_db.close()\n",
    "        print(f\"Connection to database {DB_NAME} closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert edge tree PostGIS table to file geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted PostGIS LG001_1_SKB_XYmZ into gdb C:/Users/SE1K4H/Desktop/SVK-Analys-Filer/pythonkörningar/test_postGIS\\working_250201_PostGIS.gdb.\n"
     ]
    }
   ],
   "source": [
    "# TODO Rename SKB_XYmZ feature class, dataset and table from the start to SKB_XYZ\n",
    "# TODO Delete fields like x, y, z\n",
    "# TODO check if shape should be mz or z, currently mz \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "def postGIS_to_gdb(row):\n",
    "    LG = row[\"LG\"]\n",
    "    line = row[\"line\"]    \n",
    "    table_name = f\"{LG}_{line}_SKB_XYmZ\"\n",
    "    feature_dataset_name = \"SKB_XYmZ\"\n",
    "\n",
    "    ogr_command = [\n",
    "        ogr2ogr_path,\n",
    "        \"-f\", \"OpenFileGDB\",\n",
    "        working_gdb,      \n",
    "        pg_connection,\n",
    "        \"-a_srs\", \"EPSG:3006\",\n",
    "        \"-nlt\", \"POINTZ\",\n",
    "        \"-dim\", \"3\",\n",
    "        \"-overwrite\", #TODO ändra detta när det blir flera dgn-filer, t.ex. till append\n",
    "        \"-sql\", f\"SELECT * FROM public.{table_name}\",\n",
    "        \"-nln\", table_name,\n",
    "        \"-lco\", f\"FEATURE_DATASET={feature_dataset_name}\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(ogr_command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Successfully converted PostGIS {table_name} into gdb {working_gdb}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during import: {e}\")\n",
    "\n",
    "try:\n",
    "    pg_connection = f\"PG:host={HOST} dbname={DB_NAME} user={USER} password={PASSWORD} port={PORT}\"\n",
    "    ogr2ogr_path = ogr2ogr_path\n",
    "    os.environ[\"PROJ_LIB\"] = proj_lib_path #TODO potentially solve this in another way, but needed to specify EPSG:3006\n",
    "    os.environ[\"GDAL_DATA\"] = gdal_data_path\n",
    "\n",
    "    powerlines_df = pd.read_csv(powerline_list, sep=\"\\t\", header=0)\n",
    "    powerlines_df.apply(postGIS_to_gdb, axis=1)\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slå ihop kantträd för alla ledningar till en featureclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working_gdb = \"C:\\SVK_2024\\python_work_dir\\working_250218.gdb\"\n",
    "print(working_gdb)\n",
    "#print(Path(working_gdb,\"SKB_XYZ\"))\n",
    "arcpy.env.workspace = working_gdb\n",
    "#arcpy.env.workspace = os.path.join(working_gdb,\"SKB_XYZ\")\n",
    "print(arcpy.env.workspace)\n",
    "feature_classes = arcpy.ListFeatureClasses(feature_dataset=\"SKB_XYZ\")\n",
    "print(feature_classes)\n",
    "output_feature_classes = os.path.join(working_gdb,\"kantträd_oklippta\") #\"c:\\workspace\\working_25.gdb\\merged_feature_class\"\n",
    "print(output_feature_classes)\n",
    "arcpy.Merge_management(feature_classes, output_feature_classes)\n",
    "print(\"körning av cell klar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klipp träffpunkter\n",
    "Se till att använda aktuella data i LG_polygons och station_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#fc_ledningsgator = r\"Q:\\Projekt\\Data_2024\\Underlag_SVK\\Bestallningsunderlag_2024.gdb\\GNG_LEDNINGSGATA\" #os.path.join(gdb, \"Gator2021\")\n",
    "#fc_stationsomraden = r\"Q:\\Projekt\\Data_2024\\Underlag_SVK\\Bestallningsunderlag_2024.gdb\\GNG_STATIONSOMRADE\" #os.path.join(gdb, \"STATIONSOMRADE\")\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "##fd_ej_akuta = os.path.join(gdb, \"ej_akuta\")\n",
    "#fd_all = os.path.join(gdb, \"all\")\n",
    "#fd_traffpunkter = os.path.join(gdb, \"traffpunkter\")\n",
    "\n",
    "#fc_all = os.path.join(fd_all, f\"all_{version}\")\n",
    "#fc_traffpunkter = os.path.join(fd_traffpunkter, f\"traffpunkter_{version}\")\n",
    "tmp_layer_traffpunkter = os.path.join(working_gdb, \"tmp_layer_traffpunkter\")\n",
    "print(f\"tmp_layer_traffpunkter: {tmp_layer_traffpunkter}\")\n",
    "\n",
    "fc_traffpunkter = os.path.join(results_gdb, 'kantträd')\n",
    "print(f\"fc_traffpunkter: {fc_traffpunkter}\")\n",
    "\n",
    "fc_all = os.path.join(working_gdb, \"kantträd_oklippta\")\n",
    "print(f\"fc_all: {fc_all}\")\n",
    "\n",
    "#arcpy.management.Delete(tmp_layer_traffpunkter) #behövs detta bara i fall man kör om funktionen och tmp_layer_traffpunkter existerar \n",
    "\n",
    "\n",
    "\n",
    "# Gör kopia av oklippta\n",
    "arcpy.management.CopyFeatures(fc_all, fc_traffpunkter)\n",
    "\n",
    "arcpy.management.MakeFeatureLayer(fc_traffpunkter, tmp_layer_traffpunkter)\n",
    "\n",
    "# Steg 1\n",
    "# Inom fastbredd och stationsområden: ta bort alla inom 7 m horisontellt avstånd från fas\n",
    "#\n",
    "# 1.1 Välj alla inom fastbredd\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = LG_polygons,\n",
    "                                       selection_type = \"NEW_SELECTION\")\n",
    "# 1.2 Lägg till alla inom stationsområden\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = station_polygons,\n",
    "                                       selection_type = \"ADD_TO_SELECTION\")\n",
    "# 1.3 Av de valda, behåll punkter med avst_hori < 7 m\n",
    "arcpy.management.SelectLayerByAttribute(in_layer_or_view = tmp_layer_traffpunkter,\n",
    "                                        selection_type = \"SUBSET_SELECTION\", \n",
    "                                        where_clause = '\"AVSTAND_HORISONTELLT\" < 7')\n",
    "# 1.4 Radera valda\n",
    "arcpy.management.DeleteFeatures(tmp_layer_traffpunkter)\n",
    "\n",
    "# Steg 2\n",
    "# Inom fastbredd och stationsområden: ta bort alla med trädhöjd <= 4.5 m och avst_fas >= 5 m\n",
    "#\n",
    "# 2.1 Välj alla inom fastbredd\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = LG_polygons,\n",
    "                                       selection_type = \"NEW_SELECTION\")\n",
    "# 2.2 Lägg till alla inom stationsområden\n",
    "arcpy.management.SelectLayerByLocation(in_layer = tmp_layer_traffpunkter, \n",
    "                                       overlap_type = \"INTERSECT\", \n",
    "                                       select_features = station_polygons,\n",
    "                                       selection_type = \"ADD_TO_SELECTION\")\n",
    "# 2.3 Av de valda, gör gör subset selection med punkter med trädhöjd <= 4.5 OCH avst_fas >= 5\n",
    "arcpy.management.SelectLayerByAttribute(in_layer_or_view = tmp_layer_traffpunkter,\n",
    "                                       selection_type = \"SUBSET_SELECTION\",\n",
    "                                       where_clause = '\"DELTA_HOJD\" <= 4.5 AND \"AVSTAND_FAS\" >= 5')\n",
    "# 2.4 Radera valda\n",
    "arcpy.management.DeleteFeatures(tmp_layer_traffpunkter)\n",
    "\n",
    "## Kopiera kvarvarande punkter till traffpunkter\n",
    "#arcpy.management.CopyFeatures(tmp_layer_traffpunkter, fc_traffpunkter) # Överflödigt?\n",
    "\n",
    "# Ta bort det temporära lagret\n",
    "arcpy.management.Delete(tmp_layer_traffpunkter)\n",
    "\n",
    "print(\"körning av cell klar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from datetime import date\n",
    "import arcpy\n",
    "\n",
    "#r\"C:\\SVK_2024\\python_work_dir\\results_250218.gdb\"\n",
    "#results_gdb = os.path.join(local_dir, f\"results_{run_ID}.gdb\")\n",
    "#print(Path(results_gdb))\n",
    "\n",
    "arcpy.env.workspace = working_gdb\n",
    "print(arcpy.env.workspace)\n",
    "kanttrad = os.path.join(results_gdb, 'kantträd')\n",
    "print(kanttrad)\n",
    "\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"LEDNINGSGATA\", \"cvd_LEDNINGSGATA\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"LITTERA\", \"cvd_LITTERA_LEDNING\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Matosakerhet_Hojd\", \"cvd_MATOSAKERHET\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Matosakerhet_Plan\", \"cvd_MATOSAKERHET\")\n",
    "arcpy.management.AssignDomainToField(kanttrad, \"Insamlingsmetod\", \"cvd_INMATNINGSMETOD\")\n",
    "\n",
    "\n",
    "\n",
    "#print(kanttrad)\n",
    "#arcpy.env.workspace = kanttrad\n",
    "#arcpy.env.workspace(kanttrad)\n",
    "#print(arcpy.env.workspace)\n",
    "#feature_classes = arcpy.ListFeatureClasses(feature_dataset=\"SKB_XYZ\")\n",
    "print(\"körning av cell klar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kontroll\n",
    "### Här kan man lägga in egna skript för kontroll av resultatet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLytta till RBX-notebook.\n",
    "# Listar alla LG - littera som finns i RBX-resultatet\n",
    "# Jämför detta med logg-filen för att se att rätt ledningar\n",
    "# är med i resultatet (inte alla ledningar har RBX-punkter)\n",
    "\n",
    "import arcpy\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "gdb_path = r\"C:\\SVK_2024\\pythonkörningar\\results_2501xx.gdb\"\n",
    "feature_class = \"RBX_closest_points\"\n",
    "\n",
    "gdf = gpd.read_file(gdb_path, layer=feature_class)\n",
    "\n",
    "gdf = gdf[[\"LEDNINGSGATA\", \"LITTERA\"]]\n",
    "unique_df = gdf.drop_duplicates().sort_values([\"LEDNINGSGATA\", \"LITTERA\"])\n",
    "\n",
    "print(unique_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
